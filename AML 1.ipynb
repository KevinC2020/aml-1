{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data and library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import LeaveOneOut\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import plot_confusion_matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn import preprocessing\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.tokenize.treebank import TreebankWordDetokenizer\n",
    "\n",
    "from string import punctuation\n",
    "import re\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "import transformers\n",
    "from transformers import AutoModel, BertTokenizerFast\n",
    "\n",
    "# specify GPU\n",
    "device = torch.device(\"cuda\")\n",
    "\n",
    "\n",
    "df_raw = pd.read_csv(\"utterance-channel-dataset.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Label Count</th>\n",
       "      <th>Percentage</th>\n",
       "      <th>(Word Count, count)</th>\n",
       "      <th>(Word Count, mean)</th>\n",
       "      <th>(Word Count, std)</th>\n",
       "      <th>(Word Count, min)</th>\n",
       "      <th>(Word Count, 25%)</th>\n",
       "      <th>(Word Count, 50%)</th>\n",
       "      <th>(Word Count, 75%)</th>\n",
       "      <th>(Word Count, max)</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Channel</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>General</th>\n",
       "      <td>23</td>\n",
       "      <td>7.67</td>\n",
       "      <td>987</td>\n",
       "      <td>42.913043</td>\n",
       "      <td>31.026254</td>\n",
       "      <td>13.0</td>\n",
       "      <td>20.50</td>\n",
       "      <td>34.0</td>\n",
       "      <td>55.50</td>\n",
       "      <td>116.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Introduction</th>\n",
       "      <td>35</td>\n",
       "      <td>11.67</td>\n",
       "      <td>797</td>\n",
       "      <td>22.771429</td>\n",
       "      <td>18.763365</td>\n",
       "      <td>4.0</td>\n",
       "      <td>10.50</td>\n",
       "      <td>17.0</td>\n",
       "      <td>30.50</td>\n",
       "      <td>84.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Off-topic</th>\n",
       "      <td>61</td>\n",
       "      <td>20.33</td>\n",
       "      <td>549</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>9.823441</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.00</td>\n",
       "      <td>5.0</td>\n",
       "      <td>11.00</td>\n",
       "      <td>44.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Resources</th>\n",
       "      <td>35</td>\n",
       "      <td>11.67</td>\n",
       "      <td>750</td>\n",
       "      <td>21.428571</td>\n",
       "      <td>16.398145</td>\n",
       "      <td>1.0</td>\n",
       "      <td>9.50</td>\n",
       "      <td>18.0</td>\n",
       "      <td>28.50</td>\n",
       "      <td>73.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Resume Review</th>\n",
       "      <td>25</td>\n",
       "      <td>8.33</td>\n",
       "      <td>1077</td>\n",
       "      <td>43.080000</td>\n",
       "      <td>80.962913</td>\n",
       "      <td>2.0</td>\n",
       "      <td>10.00</td>\n",
       "      <td>19.0</td>\n",
       "      <td>33.00</td>\n",
       "      <td>333.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>community-requests</th>\n",
       "      <td>40</td>\n",
       "      <td>13.33</td>\n",
       "      <td>974</td>\n",
       "      <td>24.350000</td>\n",
       "      <td>17.989384</td>\n",
       "      <td>5.0</td>\n",
       "      <td>9.00</td>\n",
       "      <td>20.0</td>\n",
       "      <td>34.25</td>\n",
       "      <td>94.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>general-questions</th>\n",
       "      <td>42</td>\n",
       "      <td>14.00</td>\n",
       "      <td>1820</td>\n",
       "      <td>43.333333</td>\n",
       "      <td>42.908676</td>\n",
       "      <td>0.0</td>\n",
       "      <td>14.25</td>\n",
       "      <td>34.5</td>\n",
       "      <td>56.25</td>\n",
       "      <td>248.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>member-generated-content</th>\n",
       "      <td>39</td>\n",
       "      <td>13.00</td>\n",
       "      <td>1531</td>\n",
       "      <td>39.256410</td>\n",
       "      <td>26.243513</td>\n",
       "      <td>2.0</td>\n",
       "      <td>20.00</td>\n",
       "      <td>35.0</td>\n",
       "      <td>51.00</td>\n",
       "      <td>127.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          Label Count  Percentage  (Word Count, count)  \\\n",
       "Channel                                                                  \n",
       "General                            23        7.67                  987   \n",
       "Introduction                       35       11.67                  797   \n",
       "Off-topic                          61       20.33                  549   \n",
       "Resources                          35       11.67                  750   \n",
       "Resume Review                      25        8.33                 1077   \n",
       "community-requests                 40       13.33                  974   \n",
       "general-questions                  42       14.00                 1820   \n",
       "member-generated-content           39       13.00                 1531   \n",
       "\n",
       "                          (Word Count, mean)  (Word Count, std)  \\\n",
       "Channel                                                           \n",
       "General                            42.913043          31.026254   \n",
       "Introduction                       22.771429          18.763365   \n",
       "Off-topic                           9.000000           9.823441   \n",
       "Resources                          21.428571          16.398145   \n",
       "Resume Review                      43.080000          80.962913   \n",
       "community-requests                 24.350000          17.989384   \n",
       "general-questions                  43.333333          42.908676   \n",
       "member-generated-content           39.256410          26.243513   \n",
       "\n",
       "                          (Word Count, min)  (Word Count, 25%)  \\\n",
       "Channel                                                          \n",
       "General                                13.0              20.50   \n",
       "Introduction                            4.0              10.50   \n",
       "Off-topic                               1.0               3.00   \n",
       "Resources                               1.0               9.50   \n",
       "Resume Review                           2.0              10.00   \n",
       "community-requests                      5.0               9.00   \n",
       "general-questions                       0.0              14.25   \n",
       "member-generated-content                2.0              20.00   \n",
       "\n",
       "                          (Word Count, 50%)  (Word Count, 75%)  \\\n",
       "Channel                                                          \n",
       "General                                34.0              55.50   \n",
       "Introduction                           17.0              30.50   \n",
       "Off-topic                               5.0              11.00   \n",
       "Resources                              18.0              28.50   \n",
       "Resume Review                          19.0              33.00   \n",
       "community-requests                     20.0              34.25   \n",
       "general-questions                      34.5              56.25   \n",
       "member-generated-content               35.0              51.00   \n",
       "\n",
       "                          (Word Count, max)  \n",
       "Channel                                      \n",
       "General                               116.0  \n",
       "Introduction                           84.0  \n",
       "Off-topic                              44.0  \n",
       "Resources                              73.0  \n",
       "Resume Review                         333.0  \n",
       "community-requests                     94.0  \n",
       "general-questions                     248.0  \n",
       "member-generated-content              127.0  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check label summary\n",
    "label_summary = df_raw.groupby(['Channel']).agg({'Utterance': 'count',\n",
    "                                           'ID':lambda x: np.round(x.count()/len(df_raw)*100,2)}) \\\n",
    "                .rename(columns={'Utterance':'Label Count','ID':'Percentage'}) \\\n",
    "#                 .sort_values(by='Label Count',ascending=False)\n",
    "# label_summary\n",
    "\n",
    "# Word count summary\n",
    "cv = CountVectorizer()\n",
    "wc = cv.fit_transform(df_raw['Utterance'])\n",
    "\n",
    "df_wc = pd.DataFrame.sparse.from_spmatrix(wc, columns= cv.get_feature_names())\n",
    "\n",
    "df_wc_channel = pd.DataFrame({'Word Count':df_wc.sum(axis=1),'Channel':df_raw['Channel']})\n",
    "\n",
    "# count words by channel\n",
    "wc_summary = df_wc_channel.groupby('Channel').describe()\n",
    "# wc_summary.columns = wc_summary.columns.droplevel(0)\n",
    "wc_summary[('Word Count', 'count')] = df_wc_channel.groupby(\"Channel\").sum()\n",
    "# wc_summary\n",
    "\n",
    "# Combine summary\n",
    "summary = pd.concat([label_summary,wc_summary],axis=1)\n",
    "summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Check top 100 words in each channel\n",
    "top100 = []\n",
    "allwords = {}\n",
    "for i in set(df_raw['Channel']):\n",
    "    cv = CountVectorizer(ngram_range=(1, 3))\n",
    "    channel_wc = cv.fit_transform(df_raw[df_raw['Channel'] == i]['Utterance'])\n",
    "    df_ct_general = pd.DataFrame.sparse.from_spmatrix(channel_wc, columns= cv.get_feature_names())\n",
    "    general_mean = df_ct_general.sum(axis=0)#.mean(axis=0)\n",
    "#     general_mean.sort_values(ascending=False)\n",
    "    top100.append(list(general_mean.nlargest(100).index.array))\n",
    "    allwords.update({i:general_mean.sort_values(ascending=False)})\n",
    "#     print(i, general_mean.nlargest(100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'about',\n",
       " 'all',\n",
       " 'am',\n",
       " 'an',\n",
       " 'and',\n",
       " 'as',\n",
       " 'but',\n",
       " 'can',\n",
       " 'data',\n",
       " 'do',\n",
       " 'for',\n",
       " 'from',\n",
       " 'have',\n",
       " 'here',\n",
       " 'hi',\n",
       " 'in',\n",
       " 'into',\n",
       " 'is',\n",
       " 'it',\n",
       " 'just',\n",
       " 'learning',\n",
       " 'like',\n",
       " 'my',\n",
       " 'of',\n",
       " 'on',\n",
       " 'the',\n",
       " 'this',\n",
       " 'to',\n",
       " 'with',\n",
       " 'you'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find top 100 words that appear in all channel \n",
    "# Can be use to create stop words \n",
    "checkset = set(top100[0]).intersection(set(top100[1]))\n",
    "for i in range(7):\n",
    "    checkset.update(checkset.intersection(set(top100[i+1])))\n",
    "checkset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Two sets are the same: True\n"
     ]
    }
   ],
   "source": [
    "# Check words appear more than 2 times in each channel\n",
    "\n",
    "top = []\n",
    "for i in set(df_raw['Channel']):\n",
    "    cv = CountVectorizer(ngram_range=(1, 3))\n",
    "    channel_wc = cv.fit_transform(df_raw[df_raw['Channel'] == i]['Utterance'])\n",
    "    df_ct_general = pd.DataFrame.sparse.from_spmatrix(channel_wc, columns= cv.get_feature_names())\n",
    "    general_mean = df_ct_general.sum(axis=0)#.mean(axis=0)\n",
    "#     general_mean.sort_values(ascending=False)\n",
    "    top.append(list(general_mean[general_mean > 1].index.array))\n",
    "#     print(i, general_mean[general_mean>1])\n",
    "\n",
    "# Find words that appear in all channel \n",
    "checkset2 = set(top100[0]).intersection(set(top100[1]))\n",
    "for i in range(7):\n",
    "    checkset2.update(checkset2.intersection(set(top100[i+1])))\n",
    "    \n",
    "# Same as top 100 words result\n",
    "print('Two sets are the same:',checkset == checkset2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the                83\n",
      "to                 63\n",
      "and                46\n",
      "of                 37\n",
      "is                 33\n",
      "                   ..\n",
      "question in         1\n",
      "query takes the     1\n",
      "query takes         1\n",
      "query               1\n",
      "01                  1\n",
      "Length: 3989, dtype: int64\n",
      "and                35\n",
      "to                 27\n",
      "from               21\n",
      "in                 20\n",
      "am                 20\n",
      "                   ..\n",
      "myself in those     1\n",
      "méxico              1\n",
      "nairobi             1\n",
      "nairobi kenya       1\n",
      "4g                  1\n",
      "Length: 1725, dtype: int64\n",
      "you                        53\n",
      "to                         35\n",
      "your                       32\n",
      "and                        26\n",
      "that                       23\n",
      "                           ..\n",
      "repost it if                1\n",
      "required                    1\n",
      "required getting            1\n",
      "required getting little     1\n",
      "03                          1\n",
      "Length: 2290, dtype: int64\n",
      "to                      24\n",
      "any                     18\n",
      "and                     18\n",
      "for                     15\n",
      "that                    14\n",
      "                        ..\n",
      "or would                 1\n",
      "or would not             1\n",
      "other good               1\n",
      "other good resources     1\n",
      "2nd                      1\n",
      "Length: 1693, dtype: int64\n",
      "the                      17\n",
      "it                       14\n",
      "is                       12\n",
      "you                      12\n",
      "and                      11\n",
      "                         ..\n",
      "rest                      1\n",
      "remember when thought     1\n",
      "remember when             1\n",
      "remember                  1\n",
      "19                        1\n",
      "Length: 1208, dtype: int64\n",
      "to                        52\n",
      "and                       26\n",
      "for                       24\n",
      "the                       23\n",
      "you                       21\n",
      "                          ..\n",
      "project will               1\n",
      "project will introduce     1\n",
      "projects and               1\n",
      "projects and community     1\n",
      "15                         1\n",
      "Length: 2077, dtype: int64\n",
      "to               39\n",
      "the              37\n",
      "we               23\n",
      "and              19\n",
      "channel          18\n",
      "                 ..\n",
      "read only         1\n",
      "read the          1\n",
      "read the wiki     1\n",
      "really            1\n",
      "28th              1\n",
      "Length: 2110, dtype: int64\n",
      "the                44\n",
      "to                 43\n",
      "of                 33\n",
      "https              33\n",
      "com                30\n",
      "                   ..\n",
      "play rock paper     1\n",
      "play rock           1\n",
      "play                1\n",
      "pixel of the        1\n",
      "03                  1\n",
      "Length: 3265, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Check words in all channels\n",
    "for i in set(df_raw['Channel']):\n",
    "    print(allwords[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Utterance</th>\n",
       "      <th>Channel</th>\n",
       "      <th>ID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>?</td>\n",
       "      <td>general-questions</td>\n",
       "      <td>64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>any ideas?</td>\n",
       "      <td>general-questions</td>\n",
       "      <td>69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>172</th>\n",
       "      <td>Samiksha Bhavsar</td>\n",
       "      <td>member-generated-content</td>\n",
       "      <td>173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>184</th>\n",
       "      <td>Thats cool</td>\n",
       "      <td>Off-topic</td>\n",
       "      <td>185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>186</th>\n",
       "      <td>oh</td>\n",
       "      <td>Off-topic</td>\n",
       "      <td>187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>192</th>\n",
       "      <td>any breakthrough?</td>\n",
       "      <td>Off-topic</td>\n",
       "      <td>193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>193</th>\n",
       "      <td>bahaha</td>\n",
       "      <td>Off-topic</td>\n",
       "      <td>194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>damn covid</td>\n",
       "      <td>Off-topic</td>\n",
       "      <td>197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>doesn't work</td>\n",
       "      <td>Off-topic</td>\n",
       "      <td>199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200</th>\n",
       "      <td>for real</td>\n",
       "      <td>Off-topic</td>\n",
       "      <td>201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>206</th>\n",
       "      <td>i hate windows</td>\n",
       "      <td>Off-topic</td>\n",
       "      <td>207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>215</th>\n",
       "      <td>LMAO</td>\n",
       "      <td>Off-topic</td>\n",
       "      <td>216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>216</th>\n",
       "      <td>lol</td>\n",
       "      <td>Off-topic</td>\n",
       "      <td>217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>219</th>\n",
       "      <td>Make him</td>\n",
       "      <td>Off-topic</td>\n",
       "      <td>220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>223</th>\n",
       "      <td>oh nice</td>\n",
       "      <td>Off-topic</td>\n",
       "      <td>224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>227</th>\n",
       "      <td>Photos.</td>\n",
       "      <td>Off-topic</td>\n",
       "      <td>228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>231</th>\n",
       "      <td>showmeyourdog</td>\n",
       "      <td>Off-topic</td>\n",
       "      <td>232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>232</th>\n",
       "      <td>shOWUSYOURDOG.</td>\n",
       "      <td>Off-topic</td>\n",
       "      <td>233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>259</th>\n",
       "      <td>hi</td>\n",
       "      <td>Resources</td>\n",
       "      <td>260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>277</th>\n",
       "      <td>any tips?</td>\n",
       "      <td>Resume Review</td>\n",
       "      <td>278</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Utterance                   Channel   ID\n",
       "63                   ?         general-questions   64\n",
       "68          any ideas?         general-questions   69\n",
       "172   Samiksha Bhavsar  member-generated-content  173\n",
       "184         Thats cool                 Off-topic  185\n",
       "186                 oh                 Off-topic  187\n",
       "192  any breakthrough?                 Off-topic  193\n",
       "193             bahaha                 Off-topic  194\n",
       "196         damn covid                 Off-topic  197\n",
       "198       doesn't work                 Off-topic  199\n",
       "200           for real                 Off-topic  201\n",
       "206     i hate windows                 Off-topic  207\n",
       "215               LMAO                 Off-topic  216\n",
       "216                lol                 Off-topic  217\n",
       "219           Make him                 Off-topic  220\n",
       "223            oh nice                 Off-topic  224\n",
       "227            Photos.                 Off-topic  228\n",
       "231      showmeyourdog                 Off-topic  232\n",
       "232     shOWUSYOURDOG.                 Off-topic  233\n",
       "259                 hi                 Resources  260\n",
       "277          any tips?             Resume Review  278"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check post length less than 3\n",
    "# Are these posts useful?\n",
    "df_raw[df_wc.sum(axis=1) < 3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Utterance</th>\n",
       "      <th>Channel</th>\n",
       "      <th>ID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>FAQ Update We've updated the Frequently Asked ...</td>\n",
       "      <td>community-requests</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>February data review We've published our Febru...</td>\n",
       "      <td>community-requests</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Privacy policy updates We've made some amendme...</td>\n",
       "      <td>community-requests</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>We have merged a new command to @Sir Lancebot:...</td>\n",
       "      <td>community-requests</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>@everyone I’m having an awesome time organizin...</td>\n",
       "      <td>General</td>\n",
       "      <td>41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>right now I've tried random slices of data fro...</td>\n",
       "      <td>general-questions</td>\n",
       "      <td>102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141</th>\n",
       "      <td>10 python hacks you must know : https://datama...</td>\n",
       "      <td>member-generated-content</td>\n",
       "      <td>142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143</th>\n",
       "      <td>Chatbots have always been so fascinating to me...</td>\n",
       "      <td>member-generated-content</td>\n",
       "      <td>144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>Finished my article on the scalping market for...</td>\n",
       "      <td>member-generated-content</td>\n",
       "      <td>146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>hello everyone, you can check out my article a...</td>\n",
       "      <td>member-generated-content</td>\n",
       "      <td>148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>Hello Everyone. I have written a blog on image...</td>\n",
       "      <td>member-generated-content</td>\n",
       "      <td>149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>hey everyone , recently i published a blog on ...</td>\n",
       "      <td>member-generated-content</td>\n",
       "      <td>150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150</th>\n",
       "      <td>Hey everyone, I just finished the first chunk ...</td>\n",
       "      <td>member-generated-content</td>\n",
       "      <td>151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>151</th>\n",
       "      <td>Hey everyone! A bunch of us got fed up with ho...</td>\n",
       "      <td>member-generated-content</td>\n",
       "      <td>152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>152</th>\n",
       "      <td>Hey everyone! This is a project of mine that I...</td>\n",
       "      <td>member-generated-content</td>\n",
       "      <td>153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154</th>\n",
       "      <td>Hi  I wrote a short article introducing \"8 Sim...</td>\n",
       "      <td>member-generated-content</td>\n",
       "      <td>155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155</th>\n",
       "      <td>Hi all, I just published an article via Toward...</td>\n",
       "      <td>member-generated-content</td>\n",
       "      <td>156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156</th>\n",
       "      <td>https://github.com/doccano/doccano - open sour...</td>\n",
       "      <td>member-generated-content</td>\n",
       "      <td>157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>157</th>\n",
       "      <td>https://myrtle.ai/how-to-train-your-resnet-8-b...</td>\n",
       "      <td>member-generated-content</td>\n",
       "      <td>158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158</th>\n",
       "      <td>https://pypi.org/project/satyrn-python/0.8.5/ ...</td>\n",
       "      <td>member-generated-content</td>\n",
       "      <td>159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159</th>\n",
       "      <td>https://towardsdatascience.com/the-viola-jones...</td>\n",
       "      <td>member-generated-content</td>\n",
       "      <td>160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>160</th>\n",
       "      <td>https://www.scratchapixel.com/lessons/mathemat...</td>\n",
       "      <td>member-generated-content</td>\n",
       "      <td>161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>161</th>\n",
       "      <td>I created an awesome list of Discord communiti...</td>\n",
       "      <td>member-generated-content</td>\n",
       "      <td>162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162</th>\n",
       "      <td>I've just written my first article on Medium, ...</td>\n",
       "      <td>member-generated-content</td>\n",
       "      <td>163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>165</th>\n",
       "      <td>Introducing pyradox: a python library that hel...</td>\n",
       "      <td>member-generated-content</td>\n",
       "      <td>166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>166</th>\n",
       "      <td>Introducing vizard: a python library that help...</td>\n",
       "      <td>member-generated-content</td>\n",
       "      <td>167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>167</th>\n",
       "      <td>Just wanted share my notes, in case they're us...</td>\n",
       "      <td>member-generated-content</td>\n",
       "      <td>168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>168</th>\n",
       "      <td>Line art with GAN. https://twitter.com/Vijish6...</td>\n",
       "      <td>member-generated-content</td>\n",
       "      <td>169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>169</th>\n",
       "      <td>Nlp repo contain all my current progress from ...</td>\n",
       "      <td>member-generated-content</td>\n",
       "      <td>170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170</th>\n",
       "      <td>python notebook tutoriel on kaggle for EDA htt...</td>\n",
       "      <td>member-generated-content</td>\n",
       "      <td>171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>171</th>\n",
       "      <td>Reformer, which was proposed in the paper Refo...</td>\n",
       "      <td>member-generated-content</td>\n",
       "      <td>172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174</th>\n",
       "      <td>So i made a deep learning project and even mad...</td>\n",
       "      <td>member-generated-content</td>\n",
       "      <td>175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175</th>\n",
       "      <td>So im finally done with this project i was wor...</td>\n",
       "      <td>member-generated-content</td>\n",
       "      <td>176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>176</th>\n",
       "      <td>Today I published an article in DataBulls. Hop...</td>\n",
       "      <td>member-generated-content</td>\n",
       "      <td>177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177</th>\n",
       "      <td>Toon-Me, Toon Portraits. GitHub link: https://...</td>\n",
       "      <td>member-generated-content</td>\n",
       "      <td>178</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Utterance  \\\n",
       "6    FAQ Update We've updated the Frequently Asked ...   \n",
       "7    February data review We've published our Febru...   \n",
       "25   Privacy policy updates We've made some amendme...   \n",
       "34   We have merged a new command to @Sir Lancebot:...   \n",
       "40   @everyone I’m having an awesome time organizin...   \n",
       "101  right now I've tried random slices of data fro...   \n",
       "141  10 python hacks you must know : https://datama...   \n",
       "143  Chatbots have always been so fascinating to me...   \n",
       "145  Finished my article on the scalping market for...   \n",
       "147  hello everyone, you can check out my article a...   \n",
       "148  Hello Everyone. I have written a blog on image...   \n",
       "149  hey everyone , recently i published a blog on ...   \n",
       "150  Hey everyone, I just finished the first chunk ...   \n",
       "151  Hey everyone! A bunch of us got fed up with ho...   \n",
       "152  Hey everyone! This is a project of mine that I...   \n",
       "154  Hi  I wrote a short article introducing \"8 Sim...   \n",
       "155  Hi all, I just published an article via Toward...   \n",
       "156  https://github.com/doccano/doccano - open sour...   \n",
       "157  https://myrtle.ai/how-to-train-your-resnet-8-b...   \n",
       "158  https://pypi.org/project/satyrn-python/0.8.5/ ...   \n",
       "159  https://towardsdatascience.com/the-viola-jones...   \n",
       "160  https://www.scratchapixel.com/lessons/mathemat...   \n",
       "161  I created an awesome list of Discord communiti...   \n",
       "162  I've just written my first article on Medium, ...   \n",
       "165  Introducing pyradox: a python library that hel...   \n",
       "166  Introducing vizard: a python library that help...   \n",
       "167  Just wanted share my notes, in case they're us...   \n",
       "168  Line art with GAN. https://twitter.com/Vijish6...   \n",
       "169  Nlp repo contain all my current progress from ...   \n",
       "170  python notebook tutoriel on kaggle for EDA htt...   \n",
       "171  Reformer, which was proposed in the paper Refo...   \n",
       "174  So i made a deep learning project and even mad...   \n",
       "175  So im finally done with this project i was wor...   \n",
       "176  Today I published an article in DataBulls. Hop...   \n",
       "177  Toon-Me, Toon Portraits. GitHub link: https://...   \n",
       "\n",
       "                      Channel   ID  \n",
       "6          community-requests    7  \n",
       "7          community-requests    8  \n",
       "25         community-requests   26  \n",
       "34         community-requests   35  \n",
       "40                    General   41  \n",
       "101         general-questions  102  \n",
       "141  member-generated-content  142  \n",
       "143  member-generated-content  144  \n",
       "145  member-generated-content  146  \n",
       "147  member-generated-content  148  \n",
       "148  member-generated-content  149  \n",
       "149  member-generated-content  150  \n",
       "150  member-generated-content  151  \n",
       "151  member-generated-content  152  \n",
       "152  member-generated-content  153  \n",
       "154  member-generated-content  155  \n",
       "155  member-generated-content  156  \n",
       "156  member-generated-content  157  \n",
       "157  member-generated-content  158  \n",
       "158  member-generated-content  159  \n",
       "159  member-generated-content  160  \n",
       "160  member-generated-content  161  \n",
       "161  member-generated-content  162  \n",
       "162  member-generated-content  163  \n",
       "165  member-generated-content  166  \n",
       "166  member-generated-content  167  \n",
       "167  member-generated-content  168  \n",
       "168  member-generated-content  169  \n",
       "169  member-generated-content  170  \n",
       "170  member-generated-content  171  \n",
       "171  member-generated-content  172  \n",
       "174  member-generated-content  175  \n",
       "175  member-generated-content  176  \n",
       "176  member-generated-content  177  \n",
       "177  member-generated-content  178  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find post with url\n",
    "httppatern = re.compile(r'http\\S+') \n",
    "pattern = []\n",
    "for i in df_raw['Utterance']:\n",
    "    pattern.append(bool(httppatern.search(i)))\n",
    "    \n",
    "df_raw[pattern]\n",
    "# Most of them are in the member-generated-content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1319185184739061760\n",
      "2d5085b9e12f\n",
      "41q9lkhiz9y\n",
      "4ceb50b84d164541bd5b0b7046aa7e6a\n",
      "4d443da2ef7d\n",
      "7357c07d8356\n",
      "74d03f5707b3b48f5d8a4c7a07299b85\n",
      "771a641c4a48\n",
      "84382d068d78\n",
      "87626aab3da3\n",
      "9a2e303d3332\n",
      "_one_page2\n",
      "abdurahman\n",
      "absolutely\n",
      "activities\n",
      "actuarial\n",
      "additional\n",
      "addresses\n",
      "administrator\n",
      "admissions\n",
      "advertising\n",
      "algorithm\n",
      "algorithmic\n",
      "algorithms\n",
      "alleviating\n",
      "alternative\n",
      "amendments\n",
      "analytics\n",
      "analyticsvidhya\n",
      "annotation\n",
      "announcements\n",
      "anonymously\n",
      "appealing\n",
      "application\n",
      "applications\n",
      "appreciate\n",
      "appreciated\n",
      "appreciates\n",
      "architecture\n",
      "artificial\n",
      "assignments\n",
      "astronomy\n",
      "attention\n",
      "attentions\n",
      "authorship\n",
      "automation\n",
      "availability\n",
      "available\n",
      "b3bcb060580\n",
      "bachelors\n",
      "background\n",
      "basically\n",
      "batch_size\n",
      "bathymetry\n",
      "bb58975e148e4c768c2f09d416baf4d2\n",
      "bd57fb8be888\n",
      "beanlearning\n",
      "beautiful\n",
      "beforehand\n",
      "beginners\n",
      "beginning\n",
      "beneficial\n",
      "betterloader\n",
      "branching\n",
      "breakthrough\n",
      "burcukoculu\n",
      "calculating\n",
      "captioning\n",
      "capturing\n",
      "catastrophic\n",
      "categorising\n",
      "celebrate\n",
      "certainly\n",
      "certification\n",
      "certifications\n",
      "certified\n",
      "charlesaverill\n",
      "clarification\n",
      "class_mode\n",
      "classical\n",
      "classification\n",
      "classifier\n",
      "classifiers\n",
      "collector\n",
      "combinations\n",
      "comfortable\n",
      "communities\n",
      "community\n",
      "competetions\n",
      "competition\n",
      "competitions\n",
      "competitive\n",
      "complaints\n",
      "completed\n",
      "completely\n",
      "complicated\n",
      "computationally\n",
      "concurrency\n",
      "condition\n",
      "confident\n",
      "connected\n",
      "considered\n",
      "constraints\n",
      "consulting\n",
      "containing\n",
      "continuous\n",
      "contributions\n",
      "conv_nets\n",
      "conversation\n",
      "conversations\n",
      "converting\n",
      "convolutional\n",
      "coordinate\n",
      "counselors\n",
      "credibility\n",
      "currently\n",
      "customizable\n",
      "databulls\n",
      "dataloader\n",
      "datamahadev\n",
      "dataprofessional\n",
      "datos_envios\n",
      "deep_learning\n",
      "deeplearning\n",
      "definitely\n",
      "deliveries\n",
      "demotivates\n",
      "dependency\n",
      "dependent\n",
      "describing\n",
      "desperate\n",
      "detailing\n",
      "detection\n",
      "determining\n",
      "developed\n",
      "developer\n",
      "development\n",
      "difference\n",
      "different\n",
      "difficult\n",
      "difficulty\n",
      "diggerdata\n",
      "discussed\n",
      "discussing\n",
      "discussion\n",
      "discussions\n",
      "disqualify\n",
      "documentation\n",
      "documents\n",
      "downgraded\n",
      "driscoll42\n",
      "e4w8w9l1mbk\n",
      "education\n",
      "efficiency\n",
      "efficient\n",
      "embeddings\n",
      "employers\n",
      "engineering\n",
      "entertaining\n",
      "enthusiast\n",
      "entrepreneurial\n",
      "envios_ts\n",
      "especially\n",
      "essentially\n",
      "evaluating\n",
      "everybody\n",
      "everything\n",
      "excellent\n",
      "exchanged\n",
      "exercises\n",
      "expectations\n",
      "expensive\n",
      "experience\n",
      "experienced\n",
      "explaining\n",
      "explanation\n",
      "extensively\n",
      "extracted\n",
      "extracting\n",
      "extractor\n",
      "extremely\n",
      "f2a6cf6a862f\n",
      "facilitate\n",
      "faraz_khan_\n",
      "fascinating\n",
      "file_train\n",
      "flow_from_directory\n",
      "fluctuating\n",
      "following\n",
      "forgetting\n",
      "formatting\n",
      "foundation\n",
      "foundations\n",
      "frameworks\n",
      "frazali52\n",
      "frequency\n",
      "frequently\n",
      "functionality\n",
      "functions\n",
      "fundamentals\n",
      "gathering\n",
      "generator\n",
      "geoscience\n",
      "graduated\n",
      "graduating\n",
      "hackathon\n",
      "handwritten\n",
      "heriberto\n",
      "highlight\n",
      "highlighting\n",
      "historical\n",
      "hoodkaggle\n",
      "hopefully\n",
      "hydration\n",
      "impactful\n",
      "implement\n",
      "implementations\n",
      "implementing\n",
      "implements\n",
      "important\n",
      "improvement\n",
      "improvements\n",
      "improving\n",
      "increased\n",
      "incredibly\n",
      "individual\n",
      "individually\n",
      "individuals\n",
      "information\n",
      "innovative\n",
      "intelligence\n",
      "intelligently\n",
      "interested\n",
      "interesting\n",
      "interests\n",
      "interfaces\n",
      "intermediate\n",
      "internship\n",
      "internships\n",
      "interviewers\n",
      "intriguing\n",
      "introduce\n",
      "introducing\n",
      "introduction\n",
      "introductory\n",
      "intuitive\n",
      "jinayshah7\n",
      "keypoints\n",
      "knowledge\n",
      "languages\n",
      "libraries\n",
      "maintenance\n",
      "manufacturing\n",
      "mathematical\n",
      "mathematics\n",
      "matplotlib\n",
      "meaningful\n",
      "memorable\n",
      "mentoring\n",
      "metricity\n",
      "microsoft\n",
      "milliseconds\n",
      "mobilenetv3\n",
      "moderating\n",
      "moderator\n",
      "mrkomododragon\n",
      "multiprocessing\n",
      "multithreading\n",
      "negatives\n",
      "negotiate\n",
      "nevermind\n",
      "normalize\n",
      "notebooks\n",
      "objective\n",
      "opportunity\n",
      "organized\n",
      "organizers\n",
      "organizing\n",
      "otherwise\n",
      "overfitting\n",
      "parameter\n",
      "participants\n",
      "participate\n",
      "particular\n",
      "performance\n",
      "permitting\n",
      "perspective\n",
      "philosophy\n",
      "pipelines\n",
      "plaintext\n",
      "pointless\n",
      "portraits\n",
      "positives\n",
      "postdistributed\n",
      "potential\n",
      "potentially\n",
      "powershell\n",
      "practical\n",
      "predictability\n",
      "predicting\n",
      "predictive\n",
      "preparing\n",
      "presentation\n",
      "pretrained\n",
      "previously\n",
      "primarily\n",
      "privately\n",
      "processes\n",
      "processing\n",
      "production\n",
      "professor\n",
      "programmer\n",
      "programming\n",
      "prominently\n",
      "promotion\n",
      "providing\n",
      "psychology\n",
      "published\n",
      "pythondiscord\n",
      "quartiles\n",
      "questions\n",
      "raspberry\n",
      "recognition\n",
      "recognize\n",
      "recognizer\n",
      "recommend\n",
      "recruiters\n",
      "recurrent\n",
      "reference\n",
      "references\n",
      "regression\n",
      "regressor\n",
      "reinforcement\n",
      "relational\n",
      "relationship\n",
      "repositories\n",
      "representing\n",
      "requested\n",
      "resolution\n",
      "resonates\n",
      "resources\n",
      "ressources\n",
      "restarting\n",
      "retinopathy\n",
      "retrieval\n",
      "reversible\n",
      "reviewing\n",
      "sagemaker\n",
      "sauravmaheshkar\n",
      "scaffolding\n",
      "scientist\n",
      "scientists\n",
      "scratchapixel\n",
      "scrolling\n",
      "searching\n",
      "segmentation\n",
      "selective\n",
      "sensitive\n",
      "sentences\n",
      "sequences\n",
      "showmeyourdog\n",
      "showusyourdog\n",
      "shuffling\n",
      "significant\n",
      "signifying\n",
      "similarity\n",
      "simulator\n",
      "solutions\n",
      "something\n",
      "sometimes\n",
      "specialization\n",
      "specifically\n",
      "specified\n",
      "speech_command_recognition_with_torchaudio\n",
      "splitting\n",
      "splotchiness\n",
      "splotchyness\n",
      "stabilization\n",
      "statement\n",
      "statements\n",
      "statistical\n",
      "statistics\n",
      "stochastic\n",
      "storytelling\n",
      "stratified\n",
      "strengths\n",
      "subfolders\n",
      "submitted\n",
      "succeeding\n",
      "successful\n",
      "successfully\n",
      "succinctly\n",
      "suffering\n",
      "suggestion\n",
      "suggestions\n",
      "suppression\n",
      "switching\n",
      "tabulating\n",
      "target_size\n",
      "technical\n",
      "techniques\n",
      "technology\n",
      "temperature\n",
      "temporarily\n",
      "tensorflow\n",
      "themselves\n",
      "tidyverse\n",
      "towardsdatascience\n",
      "train_datagen\n",
      "train_generator\n",
      "transform\n",
      "transformation\n",
      "transformer\n",
      "transformers\n",
      "transition\n",
      "tutorials\n",
      "uncomfortable\n",
      "undergrad\n",
      "underline\n",
      "understand\n",
      "understanding\n",
      "unfamiliar\n",
      "unintentional\n",
      "universities\n",
      "university\n",
      "unrelated\n",
      "utilization\n",
      "validation\n",
      "vijish68859437\n",
      "vijishmadhavan\n",
      "visualisations\n",
      "visualization\n",
      "visualizations\n",
      "waaaaaaaaaaay\n",
      "walkthrough\n",
      "warehouse\n",
      "wednesday\n",
      "wonderful\n",
      "wondering\n",
      "workflows\n",
      "yacovlewis\n",
      "yesterday\n"
     ]
    }
   ],
   "source": [
    "# Check weird token\n",
    "for i in df_wc.columns:\n",
    "    if len(i)> 8:\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "01\n",
      "02\n",
      "1319185184739061760\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "1st\n",
      "20\n",
      "200\n",
      "2019\n",
      "2020\n",
      "2025\n",
      "28\n",
      "28th\n",
      "297e\n",
      "2c3k\n",
      "2d\n",
      "2d5085b9e12f\n",
      "2nd\n",
      "30\n",
      "33\n",
      "3ish\n",
      "3rd\n",
      "400\n",
      "41q9lkhiz9y\n",
      "461\n",
      "4ceb50b84d164541bd5b0b7046aa7e6a\n",
      "4d443da2ef7d\n",
      "4g\n",
      "4k\n",
      "4th\n",
      "5000\n",
      "615k\n",
      "6900\n",
      "700kwh\n",
      "7357c07d8356\n",
      "74d03f5707b3b48f5d8a4c7a07299b85\n",
      "771a641c4a48\n",
      "84382d068d78\n",
      "87626aab3da3\n",
      "93\n",
      "944k\n",
      "95\n",
      "9a2e303d3332\n",
      "_one_page2\n",
      "aaic\n",
      "abdullah\n",
      "abdurahman\n",
      "above\n",
      "account\n",
      "acg\n",
      "achieve\n",
      "act\n",
      "activities\n",
      "actuarial\n",
      "added\n",
      "addition\n",
      "address\n",
      "adjacent\n",
      "admins\n",
      "admissions\n",
      "ads\n",
      "advertising\n",
      "advisors\n",
      "af\n",
      "afraid\n",
      "ag\n",
      "ago\n",
      "ah\n",
      "ahead\n",
      "aim\n",
      "alcohol\n",
      "algo\n",
      "algorithmic\n",
      "algorithms\n",
      "alleviating\n",
      "allowed\n",
      "allows\n",
      "already\n",
      "alright\n",
      "ama\n",
      "amendments\n",
      "analytics\n",
      "analyticsvidhya\n",
      "andre\n",
      "aniket\n",
      "annotation\n",
      "announcements\n",
      "annoying\n",
      "anomaly\n",
      "anonymously\n",
      "antonai\n",
      "anuj\n",
      "anybody\n",
      "appealing\n",
      "appended\n",
      "appreciates\n",
      "approx\n",
      "apps\n",
      "area\n",
      "areas\n",
      "aren\n",
      "ariel\n",
      "arrival\n",
      "arrivals\n",
      "arriving\n",
      "artur\n",
      "askers\n",
      "asking\n",
      "assess\n",
      "assignments\n",
      "astronomy\n",
      "ats\n",
      "attention\n",
      "attentions\n",
      "authorship\n",
      "availability\n",
      "average\n",
      "avg\n",
      "avoid\n",
      "aware\n",
      "awful\n",
      "aws\n",
      "axes\n",
      "b01\n",
      "b2b\n",
      "b3bcb060580\n",
      "bachelors\n",
      "backend\n",
      "bad\n",
      "bag\n",
      "bahaha\n",
      "bar\n",
      "bartos\n",
      "base\n",
      "bash\n",
      "bashful\n",
      "basically\n",
      "basics\n",
      "batch_size\n",
      "bathymetry\n",
      "battle\n",
      "bb58975e148e4c768c2f09d416baf4d2\n",
      "bd57fb8be888\n",
      "bean\n",
      "beautiful\n",
      "beforehand\n",
      "begin\n",
      "behind\n",
      "belong\n",
      "beneficial\n",
      "bert\n",
      "beside\n",
      "bhavsar\n",
      "binary\n",
      "binitai\n",
      "blei\n",
      "bold\n",
      "bolded\n",
      "bolivia\n",
      "bot\n",
      "both\n",
      "bots\n",
      "bounded\n",
      "boxes\n",
      "branch\n",
      "branching\n",
      "brand\n",
      "brazil\n",
      "breakthrough\n",
      "brief\n",
      "brushed\n",
      "brute\n",
      "bucket\n",
      "built\n",
      "burcukoculu\n",
      "buried\n",
      "butt\n",
      "buying\n",
      "calculating\n",
      "call\n",
      "calling\n",
      "cameras\n",
      "capstone\n",
      "captions\n",
      "capturing\n",
      "cards\n",
      "career\n",
      "carlo\n",
      "carreer\n",
      "carries\n",
      "carrying\n",
      "carter\n",
      "catastrophic\n",
      "categorising\n",
      "category\n",
      "celebrate\n",
      "cells\n",
      "centers\n",
      "certification\n",
      "certifications\n",
      "certified\n",
      "charlesaverill\n",
      "charts\n",
      "cheers\n",
      "chegg\n",
      "chemical\n",
      "cherry\n",
      "chill\n",
      "choose\n",
      "chrisjl\n",
      "christy\n",
      "chunk\n",
      "clarification\n",
      "clarify\n",
      "class_mode\n",
      "classical\n",
      "classifiers\n",
      "claudio\n",
      "clear\n",
      "clients\n",
      "clips\n",
      "cloud\n",
      "club\n",
      "cmd\n",
      "cnns\n",
      "co\n",
      "coded\n",
      "colab\n",
      "cold\n",
      "collector\n",
      "columns\n",
      "comfortable\n",
      "comment\n",
      "commerce\n",
      "company\n",
      "competetions\n",
      "competitions\n",
      "competitive\n",
      "complaints\n",
      "complete\n",
      "completely\n",
      "complicated\n",
      "compsci\n",
      "computationally\n",
      "condition\n",
      "confident\n",
      "connected\n",
      "considered\n",
      "constraints\n",
      "consulting\n",
      "contact\n",
      "contain\n",
      "contrib\n",
      "contributions\n",
      "conv_nets\n",
      "conversation\n",
      "conversations\n",
      "convert\n",
      "converting\n",
      "convnet\n",
      "coordinate\n",
      "couldn\n",
      "counselors\n",
      "covers\n",
      "covid\n",
      "cow\n",
      "creative\n",
      "credibility\n",
      "criteria\n",
      "critical\n",
      "crops\n",
      "cs224n\n",
      "current\n",
      "customer\n",
      "customizable\n",
      "cv\n",
      "cvpr\n",
      "d2l\n",
      "dad\n",
      "damn\n",
      "dataiku\n",
      "dataprofessional\n",
      "datasets\n",
      "datos_envios\n",
      "day\n",
      "dcgan\n",
      "debug\n",
      "decided\n",
      "decipher\n",
      "decoder\n",
      "decoders\n",
      "deep_learning\n",
      "deepfood\n",
      "deeplearning\n",
      "deeply\n",
      "defaults\n",
      "define\n",
      "delete\n",
      "deliveries\n",
      "deloitte\n",
      "delve\n",
      "demo\n",
      "demotivates\n",
      "dependency\n",
      "depth\n",
      "describe\n",
      "describing\n",
      "designed\n",
      "desperate\n",
      "detailing\n",
      "detect\n",
      "determining\n",
      "develop\n",
      "developed\n",
      "developer\n",
      "development\n",
      "diabetic\n",
      "dialy\n",
      "did\n",
      "difference\n",
      "difficult\n",
      "diggerdata\n",
      "digit\n",
      "dikbrown\n",
      "discount\n",
      "discuss\n",
      "discussed\n",
      "discussing\n",
      "discussions\n",
      "disqualify\n",
      "dl\n",
      "doc\n",
      "doctor\n",
      "document\n",
      "documentation\n",
      "documents\n",
      "doesnt\n",
      "dogs\n",
      "domain\n",
      "domains\n",
      "dope\n",
      "doubt\n",
      "down\n",
      "downgraded\n",
      "dr\n",
      "driscoll42\n",
      "drive\n",
      "dropout\n",
      "dropped\n",
      "ds\n",
      "dummy\n",
      "dynamic\n",
      "e4w8w9l1mbk\n",
      "earlier\n",
      "easier\n",
      "easily\n",
      "editing\n",
      "edition\n",
      "edx\n",
      "efficiency\n",
      "effort\n",
      "eg\n",
      "eh\n",
      "elements\n",
      "email\n",
      "embeddings\n",
      "emotion\n",
      "employers\n",
      "empty\n",
      "enable\n",
      "engine\n",
      "enrolled\n",
      "ensure\n",
      "enter\n",
      "entertaining\n",
      "enthusiast\n",
      "entrepreneurial\n",
      "envios\n",
      "envios_ts\n",
      "epoch\n",
      "era\n",
      "errors\n",
      "esrgan\n",
      "essentially\n",
      "evaluating\n",
      "events\n",
      "ever\n",
      "everybody\n",
      "everyday\n",
      "evilc3\n",
      "exceed\n",
      "exceeds\n",
      "excellent\n",
      "exchanged\n",
      "executed\n",
      "executes\n",
      "exercises\n",
      "expand\n",
      "expectations\n",
      "expensive\n",
      "explain\n",
      "explaining\n",
      "explains\n",
      "exposed\n",
      "extensively\n",
      "extract\n",
      "extracting\n",
      "extractor\n",
      "extreme\n",
      "extremely\n",
      "eye\n",
      "f2a6cf6a862f\n",
      "facilitate\n",
      "fair\n",
      "fall\n",
      "falls\n",
      "familiar\n",
      "fancy\n",
      "faqs\n",
      "faraz_khan_\n",
      "fascinating\n",
      "fashion\n",
      "fast\n",
      "faster\n",
      "fastly\n",
      "fc\n",
      "featured\n",
      "feed\n",
      "feels\n",
      "fight\n",
      "figured\n",
      "figures\n",
      "file_train\n",
      "finish\n",
      "firm\n",
      "fix\n",
      "flask\n",
      "flat\n",
      "flow_from_directory\n",
      "fluctuating\n",
      "flutter\n",
      "focused\n",
      "focuses\n",
      "following\n",
      "force\n",
      "foreign\n",
      "forgetting\n",
      "formal\n",
      "format\n",
      "formatting\n",
      "foundation\n",
      "foundations\n",
      "frame\n",
      "frames\n",
      "frazali52\n",
      "frequency\n",
      "fresh\n",
      "freshman\n",
      "friend\n",
      "friendly\n",
      "frontend\n",
      "fully\n",
      "function\n",
      "functionality\n",
      "fundamentals\n",
      "future\n",
      "gain\n",
      "game\n",
      "gan\n",
      "gans\n",
      "gap\n",
      "gas\n",
      "gathering\n",
      "gcp\n",
      "generator\n",
      "genre\n",
      "geoscience\n",
      "gil\n",
      "girl\n",
      "glance\n",
      "glass\n",
      "global\n",
      "glowing\n",
      "goin\n",
      "gold\n",
      "gonna\n",
      "gov\n",
      "gpas\n",
      "grab\n",
      "grading\n",
      "graduate\n",
      "graduating\n",
      "graphics\n",
      "greeting\n",
      "ground\n",
      "grouped\n",
      "groups\n",
      "growing\n",
      "guess\n",
      "guest\n",
      "guided\n",
      "guides\n",
      "guy\n",
      "hackathon\n",
      "haha\n",
      "hahaha\n",
      "handful\n",
      "handle\n",
      "handling\n",
      "hands\n",
      "handwritten\n",
      "happen\n",
      "happens\n",
      "hashing\n",
      "havent\n",
      "having\n",
      "healthy\n",
      "hear\n",
      "hei\n",
      "hell\n",
      "hella\n",
      "hemlock\n",
      "heriberto\n",
      "higher\n",
      "highest\n",
      "highlighting\n",
      "historical\n",
      "hit\n",
      "hiya\n",
      "hobby\n",
      "hola\n",
      "holi\n",
      "holy\n",
      "honest\n",
      "honestly\n",
      "hoodkaggle\n",
      "hop\n",
      "hopefully\n",
      "hopes\n",
      "hoping\n",
      "hours\n",
      "however\n",
      "html\n",
      "hug\n",
      "human\n",
      "humanity\n",
      "hydration\n",
      "idk\n",
      "iiot\n",
      "impactful\n",
      "implement\n",
      "implementations\n",
      "implements\n",
      "improve\n",
      "improved\n",
      "improvement\n",
      "improvements\n",
      "include\n",
      "incredibly\n",
      "indicate\n",
      "individual\n",
      "individuals\n",
      "industry\n",
      "info\n",
      "initial\n",
      "innovative\n",
      "insight\n",
      "inspo\n",
      "intelligently\n",
      "interest\n",
      "interesting\n",
      "interests\n",
      "interfaces\n",
      "internet\n",
      "internship\n",
      "internships\n",
      "intimate\n",
      "intriguing\n",
      "introductory\n",
      "intros\n",
      "intrude\n",
      "involve\n",
      "io\n",
      "iot\n",
      "iterate\n",
      "jason\n",
      "jetson\n",
      "jinayshah7\n",
      "job\n",
      "jobs\n",
      "join\n",
      "joined\n",
      "joining\n",
      "jon\n",
      "journal\n",
      "json\n",
      "judging\n",
      "kawtare\n",
      "ke\n",
      "kelvin\n",
      "kenya\n",
      "kernel\n",
      "kernels\n",
      "keypoints\n",
      "keyword\n",
      "keywords\n",
      "khan\n",
      "kinda\n",
      "kinds\n",
      "kronifer\n",
      "label\n",
      "labeled\n",
      "labelled\n",
      "labelme\n",
      "lancebot\n",
      "language\n",
      "languages\n",
      "last\n",
      "law\n",
      "learned\n",
      "lecture\n",
      "lectures\n",
      "legit\n",
      "lena\n",
      "lessons\n",
      "lewis\n",
      "licence\n",
      "life\n",
      "limited\n",
      "linkedin\n",
      "live\n",
      "lmao\n",
      "loan\n",
      "locality\n",
      "lock\n",
      "london\n",
      "looked\n",
      "lose\n",
      "lots\n",
      "lovely\n",
      "lsh\n",
      "lstm\n",
      "luck\n",
      "mahajan\n",
      "maintenance\n",
      "majors\n",
      "makes\n",
      "malaysia\n",
      "manufacturing\n",
      "marks\n",
      "mathematical\n",
      "maths\n",
      "matplotlib\n",
      "matrix\n",
      "mayowa\n",
      "mds\n",
      "meaningful\n",
      "means\n",
      "measure\n",
      "meeting\n",
      "members\n",
      "memorable\n",
      "memory\n",
      "mentor\n",
      "mentoring\n",
      "merged\n",
      "mert\n",
      "meta\n",
      "metricity\n",
      "mhxion\n",
      "michael\n",
      "microsoft\n",
      "midi\n",
      "mile\n",
      "million\n",
      "milliseconds\n",
      "mind\n",
      "minutes\n",
      "miss\n",
      "missed\n",
      "missing\n",
      "mnist\n",
      "mobilenetv3\n",
      "moderate\n",
      "moderating\n",
      "moderator\n",
      "modmail\n",
      "monte\n",
      "month\n",
      "morning\n",
      "mostly\n",
      "mouse\n",
      "mrkomododragon\n",
      "msc\n",
      "msds\n",
      "muddy\n",
      "multiprocessing\n",
      "multithreading\n",
      "mute\n",
      "myrtle\n",
      "méxico\n",
      "nairobi\n",
      "nano\n",
      "nasty\n",
      "natural\n",
      "negatives\n",
      "negotiate\n",
      "neperian\n",
      "net\n",
      "netomi\n",
      "nets\n",
      "nevermind\n",
      "news\n",
      "niche\n",
      "nlp_v2\n",
      "nn\n",
      "noise\n",
      "none\n",
      "normalize\n",
      "normally\n",
      "nosql\n",
      "notebooks\n",
      "noticed\n",
      "numbers\n",
      "numpy\n",
      "nvidia\n",
      "ny\n",
      "objective\n",
      "obvious\n",
      "ocr\n",
      "oil\n",
      "ok\n",
      "ones\n",
      "online\n",
      "onto\n",
      "oops\n",
      "opencv\n",
      "opportunity\n",
      "optimal\n",
      "options\n",
      "organized\n",
      "organizers\n",
      "oriented\n",
      "os\n",
      "ot0\n",
      "outcomes\n",
      "outline\n",
      "outside\n",
      "overview\n",
      "page\n",
      "pain\n",
      "pandas\n",
      "parameter\n",
      "participants\n",
      "participate\n",
      "particular\n",
      "passion\n",
      "patchy\n",
      "patterns\n",
      "paused\n",
      "pavan\n",
      "paypal\n",
      "pdfs\n",
      "perform\n",
      "performance\n",
      "permitting\n",
      "person\n",
      "perspective\n",
      "pharma\n",
      "philosophy\n",
      "phone\n",
      "pi\n",
      "pine\n",
      "ping\n",
      "pipeline\n",
      "pipelines\n",
      "pitch\n",
      "pixel\n",
      "plaintext\n",
      "plan\n",
      "platform\n",
      "play\n",
      "playing\n",
      "pleasure\n",
      "plot\n",
      "plotted\n",
      "plus\n",
      "point\n",
      "pointless\n",
      "points\n",
      "poland\n",
      "poppa\n",
      "popular\n",
      "portraits\n",
      "positives\n",
      "postdistributed\n",
      "posting\n",
      "potential\n",
      "potentially\n",
      "powerbi\n",
      "practical\n",
      "predictability\n",
      "predicting\n",
      "prepared\n",
      "preparing\n",
      "present\n",
      "presentation\n",
      "pretrained\n",
      "preview\n",
      "previously\n",
      "price\n",
      "primarily\n",
      "priority\n",
      "privately\n",
      "proceed\n",
      "processes\n",
      "prof\n",
      "professor\n",
      "profile\n",
      "profit\n",
      "programmer\n",
      "progress\n",
      "prominently\n",
      "promotion\n",
      "proposes\n",
      "proud\n",
      "provided\n",
      "providing\n",
      "prs\n",
      "pt\n",
      "public\n",
      "puppy\n",
      "purchase\n",
      "pure\n",
      "putting\n",
      "pyfact\n",
      "pypi\n",
      "pyplot\n",
      "quartile\n",
      "quartiles\n",
      "query\n",
      "quincy\n",
      "quinten\n",
      "quite\n",
      "r4ds\n",
      "range\n",
      "rapidly\n",
      "rare\n",
      "raspberry\n",
      "rather\n",
      "reaches\n",
      "reason\n",
      "recent\n",
      "recognize\n",
      "recognizer\n",
      "recruiters\n",
      "recurrent\n",
      "redacted\n",
      "reduce\n",
      "reference\n",
      "regressor\n",
      "reinforcement\n",
      "relate\n",
      "relationship\n",
      "removed\n",
      "removing\n",
      "rename\n",
      "renamed\n",
      "renaming\n",
      "render\n",
      "reply\n",
      "repositories\n",
      "representing\n",
      "requested\n",
      "requests\n",
      "required\n",
      "resolution\n",
      "resonates\n",
      "ressources\n",
      "rest\n",
      "restarting\n",
      "retinopathy\n",
      "retrain\n",
      "retrieval\n",
      "reversible\n",
      "reviewing\n",
      "rgb\n",
      "richard\n",
      "rigid\n",
      "risk\n",
      "rnn\n",
      "roadmap\n",
      "rocks\n",
      "rohan\n",
      "roles\n",
      "rolled\n",
      "rolling\n",
      "rstudio\n",
      "rtx\n",
      "runtime\n",
      "rx\n",
      "sad\n",
      "safe\n",
      "sales\n",
      "samiksha\n",
      "sauravmaheshkar\n",
      "save\n",
      "says\n",
      "scaffolding\n",
      "scalpers\n",
      "scenario\n",
      "scene\n",
      "scientist\n",
      "scratchapixel\n",
      "scrolling\n",
      "seaborn\n",
      "search\n",
      "searching\n",
      "seeing\n",
      "seeking\n",
      "selective\n",
      "sending\n",
      "sensitive\n",
      "sentences\n",
      "sequence\n",
      "sequences\n",
      "serve\n",
      "servers\n",
      "setup\n",
      "several\n",
      "severe\n",
      "severity\n",
      "she\n",
      "shitpost\n",
      "short\n",
      "show\n",
      "showcase\n",
      "showing\n",
      "showmeyourdog\n",
      "showusyourdog\n",
      "shreyz\n",
      "shuffling\n",
      "sided\n",
      "sigh\n",
      "sigmoid\n",
      "sign\n",
      "significant\n",
      "signifying\n",
      "similar\n",
      "simply\n",
      "simulator\n",
      "single\n",
      "sir\n",
      "sister\n",
      "sk\n",
      "skeewed\n",
      "skill\n",
      "sklearn\n",
      "slot\n",
      "slower\n",
      "slowmode\n",
      "sm\n",
      "smaller\n",
      "smallest\n",
      "smart\n",
      "sme\n",
      "snack\n",
      "snoopy\n",
      "social\n",
      "softmax\n",
      "sold\n",
      "solution\n",
      "somebody\n",
      "somenoe\n",
      "speak\n",
      "specialization\n",
      "specifically\n",
      "specified\n",
      "speech_command_recognition_with_torchaudio\n",
      "speeding\n",
      "split\n",
      "splitting\n",
      "splotchiness\n",
      "splotchyness\n",
      "sql\n",
      "stabilization\n",
      "star\n",
      "starter\n",
      "statements\n",
      "statistical\n",
      "statistics\n",
      "status\n",
      "stem\n",
      "stochastic\n",
      "stock\n",
      "stopped\n",
      "storage\n",
      "stored\n",
      "stories\n",
      "storytelling\n",
      "stranger\n",
      "stratified\n",
      "stratify\n",
      "streams\n",
      "street\n",
      "strengths\n",
      "striving\n",
      "struck\n",
      "stuck\n",
      "studied\n",
      "studying\n",
      "subfolders\n",
      "submitted\n",
      "subset\n",
      "subtract\n",
      "succeeding\n",
      "success\n",
      "successful\n",
      "successfully\n",
      "succinctly\n",
      "suffering\n",
      "summary\n",
      "sunday\n",
      "super\n",
      "support\n",
      "supports\n",
      "suppression\n",
      "surely\n",
      "survey\n",
      "sutton\n",
      "swear\n",
      "switching\n",
      "synaptic\n",
      "tabulating\n",
      "tackle\n",
      "tag\n",
      "tailgate\n",
      "taken\n",
      "taking\n",
      "talked\n",
      "talks\n",
      "talmud\n",
      "target_size\n",
      "targets\n",
      "task\n",
      "taught\n",
      "tbh\n",
      "technical\n",
      "technology\n",
      "temporarily\n",
      "term\n",
      "terror\n",
      "textbook\n",
      "texts\n",
      "theme\n",
      "themselves\n",
      "theres\n",
      "thing\n",
      "thinking\n",
      "thirsty\n",
      "threads\n",
      "three\n",
      "thumb\n",
      "tidyverse\n",
      "timers\n",
      "times\n",
      "tl\n",
      "tomorrow\n",
      "tonight\n",
      "tool\n",
      "toolbox\n",
      "totally\n",
      "track\n",
      "tracking\n",
      "train_datagen\n",
      "train_generator\n",
      "trains\n",
      "transfer\n",
      "transform\n",
      "transformation\n",
      "trap\n",
      "trash\n",
      "trax\n",
      "tricks\n",
      "trio\n",
      "triplet\n",
      "troubles\n",
      "trust\n",
      "turkey\n",
      "tutorials\n",
      "tutoriel\n",
      "tw\n",
      "twisted\n",
      "twitter\n",
      "ubuntu\n",
      "ui\n",
      "unable\n",
      "uncomfortable\n",
      "under\n",
      "understanding\n",
      "unfamiliar\n",
      "uni\n",
      "unintentional\n",
      "union\n",
      "unique\n",
      "universities\n",
      "university\n",
      "unless\n",
      "unrelated\n",
      "until\n",
      "updated\n",
      "upon\n",
      "upper\n",
      "upscale\n",
      "utica\n",
      "utilized\n",
      "valuable\n",
      "vanilla\n",
      "various\n",
      "vector\n",
      "vega\n",
      "verify\n",
      "vijish68859437\n",
      "vijishmadhavan\n",
      "vineesh\n",
      "violated\n",
      "vishwas\n",
      "visualisations\n",
      "visualization\n",
      "visualizations\n",
      "visually\n",
      "vivek\n",
      "voice\n",
      "vote\n",
      "waaaaaaaaaaay\n",
      "walkthrough\n",
      "wanna\n",
      "wants\n",
      "warehouse\n",
      "wars\n",
      "wasn\n",
      "watch\n",
      "watching\n",
      "weakness\n",
      "weather\n",
      "web\n",
      "webpage\n",
      "wednesday\n",
      "were\n",
      "wet\n",
      "whenever\n",
      "wheras\n",
      "whether\n",
      "white\n",
      "width\n",
      "wild\n",
      "wildly\n",
      "winkey\n",
      "wish\n",
      "wolrd\n",
      "wonder\n",
      "workflow\n",
      "workflows\n",
      "world\n",
      "worse\n",
      "worth\n",
      "writer\n",
      "writing\n",
      "xdg\n",
      "xt\n",
      "xtest\n",
      "xtrain\n",
      "xyz\n",
      "ya\n",
      "yacovlewis\n",
      "yep\n",
      "yesterday\n",
      "yolo\n",
      "yolov3\n",
      "youd\n",
      "youre\n",
      "yourself\n",
      "yt\n",
      "ytest\n",
      "ytrain\n",
      "yuki\n"
     ]
    }
   ],
   "source": [
    "# Check words that only appears once \n",
    "for i in df_wc.sum()[df_wc.sum() < 2].index:\n",
    "    print(i)\n",
    "    \n",
    "# this may get a idea how to fix some of the words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['about', 'and', 'around', 'asked', 'centers', 'com', 'common', 'community', 'faq', 'frequently', 'get', 'https', 'link', 'now', 'on', 'our', 'page', 'pages', 'pythondiscord', 'questions', 'server', 'tag', 'the', 'this', 'to', 'update', 'updated', 've', 'we', 'webpage', 'website', 'will']\n",
      "['!faq', 'about', 'and', 'around', 'asked', 'centers', 'common', 'community.', 'faq', 'frequently', 'frequently-asked-questions', 'get', 'https:', 'link', 'now', 'on', 'our', 'page', 'pages', 'pythondiscord.com', 'questions', 'server', 'tag', 'the', 'this', 'to', 'update', 'updated', 've', 'we', 'webpage.', 'website:', 'will']\n",
      "['community', 'website', 'pythondiscord', 'https', 'com', 'webpage', '!faq', 'frequently-asked-questions', 'https:', 'pythondiscord.com', 'community.', 'website:', 'webpage.']\n"
     ]
    }
   ],
   "source": [
    "# New token pattern\n",
    "cv = CountVectorizer()\n",
    "cv.fit_transform(df_raw['Utterance'].iloc[6:7])\n",
    "\n",
    "print(cv.get_feature_names())\n",
    "\n",
    "cv2 = CountVectorizer(token_pattern = '[a-zA-Z0-9$&+,:;=?@#|<>.^*()%!-]+')\n",
    "cv2.fit_transform(df_raw['Utterance'].iloc[6:7])\n",
    "\n",
    "print(cv2.get_feature_names())\n",
    "\n",
    "def Diff(li1, li2):\n",
    "    return (list(list(set(li1)-set(li2)) + list(set(li2)-set(li1))))\n",
    "\n",
    "print(Diff(cv.get_feature_names(),cv2.get_feature_names()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Based on the checkset above \n",
    "custStopList = ['all', 'and', 'as', 'be', 'can', 'everyone', 'for', 'have', 'if', 'in', 'it', 'just',\n",
    " 'like', 'me', 'of', 'on', 'or', 'some', 'that', 'the', 'this', 'to', 'with', 'would', 'you']\n",
    "\n",
    "customStopWords = set(stopwords.words('english')+custStopList)\n",
    "ls_remove = ['how','what','when','who','why']\n",
    "customStopWords.difference_update(ls_remove)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "customStopWords = set(stopwords.words('english')+list(punctuation))\n",
    "ls_remove = ['?', 'how','what','when','who','why']\n",
    "customStopWords.difference_update(ls_remove)\n",
    "\n",
    "wnl = WordNetLemmatizer()\n",
    "porter_stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean up text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Separate Category for the Weekly Topic To better highlight the weekly topic, we've moved the weekly topic channel to a separate channel. This also allowed us to have two separate text channels, one highlighting the topic (#weekly-topic) and one for discussing the topic (#weekly-topic-discussion).(edited)\n",
      "hey, i would like to ask if it's possible to get the channel where users exchanged projects back\n",
      "Any update on my ModMail about the hackathon?\n",
      "@Admins I have a suggestion. Python has some music libraries and also some great features like MIDI Handling. It would be a good idea if we opened a music in python topic channel.\n",
      "Can we have a beginner channel\n",
      "Channel Rename We've changed #async-and-concurrency from #async to what it is currently. This is to better indicate that all concurrency topics (i.e. threads, trio, twisted, multiprocessing, GIL) belong in this topical channel, not just async.\n",
      "FAQ Update We've updated the Frequently Asked Questions page on our website: https://pythondiscord.com/pages/frequently-asked-questions/ The FAQ now centers around common questions we get about this server and community. The !faq tag will link to this webpage.\n",
      "February data review We've published our February data review containing information on the data we store to run the community. Find it here: https://www.notion.so/pythondiscord/Sunday-28th-February-bb58975e148e4c768c2f09d416baf4d2(edited)\n",
      "Food names We've changed the names of our help channels from chemical elements to food. We hope these will be more memorable for users of the help system.\n",
      "Hey, I see you asked in #python-general. Python-general is a rapidly scrolling discussion channel, so sometimes your questions will be missed.\n",
      "I don't feel comfortable with product advertising in #resources\n",
      "I have an idea, after a recent update when we send a file in discord there is a preview of the content, so why not make it so that if we type !e and send the file it executes it\n",
      "I suggest a self-promotion channel where anyone can shitpost about themselves and the people who don't care can mute it\n",
      "I would be down for a little group showing off of people's personal projects though. Good for inspo for the newer people and I don't really understand what half of you do\n",
      "I've seen some other servers add functionality so you can't post unless you perform some action specified in the wiki. That might be useful to ensure question askers actually read the wiki before posting.\n",
      "Icon & Banner Change The server icon and banner have been changes to celebrate Holi. Thanks to @AG for the icon & banner!\n",
      "idk what other topics youd have, just some suggestions\n",
      "Is it possible to lower slowmode in #python-general?\n",
      "Is this the right Channel to ask for help/hints for planning a project?\n",
      "It'll be useful to probably have maybe a link to a beginner's guide/roadmap for ML if people want to figure out how to learn.\n",
      "It'll probably be nice to have more guides on specific topics and maybe start tabulating FAQs?\n",
      "Itd be like a discord server event\n",
      "Lower the sm for #ot0-hemlock’s-morning-deliveries?\n",
      "Maybe a bit silly, but would it be possible to have a dog pictures channel? I'd love to share photos of mine but don't want to intrude on the cat theme :)\n",
      "New Development Channel: #dev-bounty-board We just opened the #dev-bounty-board channel that list our top priority GitHub issues and PRs on our project. If you are interested in helping out, feel free to check out the listed issues and send a message in #dev-contrib, we will help you get started!\n",
      "Privacy policy updates We've made some amendments to our privacy policy to clarify data storage by Metricity, our Discord metric collector. Please find the new version at: https://www.notion.so/pythondiscord/Privacy-Policy-4ceb50b84d164541bd5b0b7046aa7e6a(edited)\n",
      "question for ya. I stream in twitch in the Science & Technology genre, Machine learning studying and related data science stuff. is there a way we can have a stream-announcements channel that we can post our twitch channel in this discord server?\n",
      "Renaming #data-science-and-ai We've renamed the data-science channel to #data-science-and-ai (data-science-and-ai). This is meant to help beginners more easily understand the topic at first glance.\n",
      "should we have topical channels instead of or in addition to just #machine-learning ?\n",
      "so, I am a little lost here! seems to me that there's one project being worked on by @yacov but I was wondering if we could create more project channels over here by the experienced folks probably so that the people arriving can follow along a project if they are interested. I sure am interested as hell in working on a project related to computer vision besides building my resume to be honest, and I am here for a mentor or a team I could work with for the same. I don't have a project idea, but if experienced folks have any, please let me know!\n",
      "The useless pointless questions are kinda annoying\n",
      "there should be a student role!\n",
      "There's so many \"I've started in data science, what should I learn\" type questions.\n",
      "think there should be some channels specific to languages.\n",
      "We have merged a new command to @Sir Lancebot: .pyfact. It give you facts... about python :). Thanks to @MrKomodoDragon for writing it, and @Chrisjl @Kronifer and @Jason Terror ♦ for reviewing it. It currently only has a few facts, so please suggest any you'd like added on the discussion here: https://github.com/python-discord/meta/discussions/93(edited)\n",
      "what should one do when rule 1 gets violated?\n",
      "What was that idea a while back, like a channel people get put in to start with or something\n",
      "What would be some must have general pages/sections for a community wiki to you guys?\n",
      "What's the #sister-communities channel for? It's empty and read-only.\n",
      "Would it be a good idea to have some kind of small project we all work on together? Like maybe we all enter a kaggle competition, try to do something cool/niche(edited)\n",
      "@everyone I’m having an awesome time organizing this community with y’all and I’m excited to being putting these projects together with your help. As I organize these 4 projects, I figure it’ll be helpful for me to set up some time with some of you individually to better organize these projects, help people form groups, and make it easier for me to help you find jobs at the end of the project if that’s relevant for you. I set up some availability tomorrow and on Wednesday to get to know some folks and learn more about what will work best. Feel free to grab a 15 minute slot with me at this link: https://calendly.com/yacovlewis/introduction \n",
      "@everyone Introducing Data Science with Expert Guidance Project 1: Applications in ML (AML)! For everyone at other levels, I will be rolling out those projects over the next week or two. For Project 1: AML, participants will build a discord moderator bot that helps members post in the right channel. This project will introduce people to NLP, ML classifiers, git, and web applications if time permitting. For a full walkthrough of the project and to sign up, go over to the #apps-in-ml-1 channel in the open projects section. The introductory post there will answer many of your questions. If you still have questions, feel free to post them there. I'm looking forward to starting this first journey with you guys!\n",
      "@everyone the results are in and the most requested experience is \"Projects for different difficulty levels for non-experts to carry out.\" Please help me outline projects that would be valuable to you. I'll share my initial thoughts around what these projects will look like and what they should provide. Please give me feedback on these ideas to help me best facilitate this experience for you all.\n",
      "1. Most advanced: New topics within Deep Learning. Examples could be computer vision, AI for Audio, AI for Video, Natural Language Processing, etc.\n",
      "2.  Projects for different difficulty levels for non-experts to carry out. Tackle a project individually or in a group. Experts will be available to answer questions around the project. Examples of projects could be: kaggle projects, projects for real world use, dummy projects for beginners, etc\n",
      "2. Advanced: Introduction to deep learning. A deep learning project with the goal of introducing people to deep learning concepts(edited)\n",
      "3.  Projects led by experts for non-experts to watch. Follow along as an expert carries out a project to see how they do it, and how they learn along the way. Examples are the project I'm carrying out in audio, (this project is temporarily paused as I negotiate with my client about what I can share publicly. Updates will come soon). See how medium articles or open source projects are executed from beginning to end.\n",
      "3. Intermediate: Applications in Machine Learning. A Machine learning project with the goal of helping people apply machine learning concepts and learning a new domain of machine learning(edited)\n",
      "4.  Guest audio sessions, AMA's etc from experts on certain topics in a voice channel at a specific time. Sessions could be led by experts in specific domains of AI, or could come from recruiters for data science, admissions counselors at universities, etc.(edited)\n",
      "4. Beginner: Introduction to Machine learning. A machine learning project with the goal of introducing people to machine learning concepts\n",
      "Alright I'll figure some things out  Please feel free to reach out to me to help, share ideas, say hi, or whatever you want. I'm just trying to help and I'm learning a lot in the process\n",
      "Calendly Introduction - Yacov Lewis If there's anything you want me to look at beforehand, just send it to me ahead of our call and I'll take a look!\n",
      "Going to repost what got buried under all these arrival messages: Thank you all for joining  I’m certainly new to organizing a discord community but I can see that many people would really deeply benefit from going through their data science journey with guidance from experts they can trust so thats what I’m trying to do here. I’ll try and be as available as I can for the community and help coordinate with other DS experts to make this a great place for people to learn and become big time data scientists\n",
      "Help us make this community more of what you need. Please react to the ideas that I post here or that others comment that you would most like to see as part of this server:\n",
      "Help: If you're a data science expert and want to help review and moderate answers and resources, answer questions, or share projects, please dm me If you have experience moderating a community, and you want to help with this community, please dm me Since I'm new to this, I've created a channel called community-requests-and-feedback to post what you think the community organizers should do or improvements they should make. Also feel free to just dm me whatever you feel like saying (please be nice  )\n",
      "Hey @yacov, I’m interested in 1 “new topics”. I’d be interested in doing something in NLP. I’ve worked on a chatbot before 3ish years ago in grad school and it didn’t work well. Would love to try again with better data, and newer models like transformers.\n",
      "Once again, I’m new to this whole process and trying my best to help, so if anyone has suggestions, complaints, advice etc just let me know\n",
      "Please vote for the difficulty of the project that you would like to participate in:\n",
      "Super cool idea! Others should please react to this suggestion or post others as well. As soon as I see some interest around some of these options i’ll figure out how to make them happen!\n",
      "Please keep this channel only for information related to projects and community issues that are relevant to everyone\n",
      "Thank you all for you help in making this community so wonderful for all of us\n",
      "I want this to be a place where people can come and feel confident in what they're learning and feel safe asking their questions\n",
      "The welcome messages have now been moved to arrivals. Thank you @Papa Kamis\n",
      "?\n",
      "#deep_learning #conv_nets how to choose networks depth and width for a given input resolution?\n",
      "1. what should the audio I record be like, eg. one continuous stream of me saying wake up over and over, vs one of me not saying it or saying different things, or one separate audio file for me saying wake up once per training example and one audio file of me saying something else per example? 2. what should the actual examples look like? random slices of data from the dataset, or actual full \"wake up\" clips to prevent the network from learning \"ke up\" or \"wake\" on their own? 3. how much data should I have, and what kind of model is the smallest possible?\n",
      "Additional question, is splitting date into year/month/day better than just converting it into milliseconds? Additional context: I'm working with stock historical data.\n",
      "Also, how do you normalize year? Wouldn't it make no sense since it would not take succeeding years into account?(edited)\n",
      "any ideas?\n",
      "any rule of thumb for guessing a good network size?\n",
      "basically i want to learn how fast the temperature changes without learning any patterns\n",
      "does anyone has any experience with mds?\n",
      "does anyone know how to do image embeddings without labels which doesnt involve just a CNN and triplet loss\n",
      "Hello I'm new here and stuck on something - any insight appreciated. I can see via bar charts that when X falls within upper and lower quartiles I get significant change in the avg value of y (which is a % representing risk). I have around 10 such features I am evaluating. Using Random Forest Regressor (sklearn), I feed continuous X data (better result) and also try categorising X by quartile bucket (worse result). Am I using the right algo for for this scenario please? Can random forest understand that my features are predictive only when they have extreme values (I guess this is a non linear relationship)\n",
      "Hello. I’m new here and wanted to ask a question specifically about feature engineering. When I google this topic, articles like to highlight that it is an extremely important part of machine learning (I would agree with that) but then they go on to focus on topics like data cleaning (dealing with missing data) extracting information (from names or street addresses), adding data (like if you have a location and time, you could add the weather) or removing useless data that can muddy the model. Certainly all important topics but there is one area I’m interested in that does not seem to be discussed. In one of the Kaggle learn sections they mention how one of the practice competitions is the loan defaults data set. The competition is designed to be setup as a black box where we are not meant to have an intimate knowledge of the data to build new features. They mention that someone figured out that, out of the hundreds of features, if you subtract 1 specific one from another, you created a new feature with high predictability. But they never discuss HOW it was found that those feature combinations had the dependency. So, TL;DR if you have a data set with hundreds or more columns, where you do not have an intuitive knowledge of the information, how can you go about finding potentially impactful combinations. I’m sure you could brute force it but if the data set was large enough, this would become computationally expensive right? Is there a statistical method for determining this?\n",
      "Hey guys I have small engine here where I record the temperature and load every minute. load increase -> temperature increase (slower) The thing is the temperature will be around roughly the same with a given load. but is there a way i can forecast the temperature ?\n",
      "Hey guys quick question. I’ve been looking through a lot of Keras models for LSTM and noticed they all have 2 train and test datasets labelled xtrain ytrain and xtest and ytest. What do each of these usually mean? Why can’t we just have all the data in one set? Why do we need and x and a y?\n",
      "Hey guys, I plotted the loss, accuracy and validation accuracy per epoch, and I was just wondering if anyone had any ideas as to why the validation accuracy is fluctuating so wildly. I am absolutely aware that the model is fairly awful, just wondering as to why to it goes from 90 - 50 - 90 - 50... so often\n",
      "Hey guys, I was playing around with train-test-split with the stratify parameter vs Stratified Shuffling and I was wondering if there was any difference between them?\n",
      "Hey, I'm trying to upscale high res images using ESRGAN (by high res I mean ~4K images) but with pretrained model there's no improvement. Is it because network wasn't trained with large images ?\n",
      "Hi ! i'm working in facial emotion detection with a new dataset .. But i'm a little bit lost .. If someone would like to help me, it'll be a pleasure and a great help hahaha ! I'm a girl from France , I study psychology and artificial intelligence .. Thank you !!\n",
      "hi everyone ! im dealing with a forecast time series in r... i hope somenoe could help me. i have troubles to set the ts.... my query takes the data dialy, from yesterday to 56 days before, and my script to set the ts is : envios_ts <- ts(datos_envios$envios, start = c(2021,01),end = c(2021,56), frequency = 14) but when i plot this ts, the x axes goes to 2025.\n",
      "hi everyone... this question may be silly, but im new in this wolrd so i have licence to be silly. is there a way to avoid neperian numbers like \"p-value: 5.297e-10\"\n",
      "hi guys, i have a question that i hope someone can help me solve this problem. I'm doing my capstone project, the project is about smart door lock using facial recognition. I have already built the API using Flask and OpenCV for capturing faces and training. After that, I don't know how to put the model to the Raspberry Pi for facial recognition. If you have any ideas, please let me know. Thanks for your help guys.\n",
      "Hi guys, i trained a model on 50 classes of deepfood-101 end to end. Then I took out the dense layers and ended with GAP -> extracted features for training, testing and validation data. In order to be able to retrain the model for more classes, I took the extractor, extracted features for new classes (still food) and tried to train Dense layers on the union of old and new extracted data. If I try that with only old data, it trains approx to the same state as it was before. If I add the new classes, it just goes to accuracy of 0.02. I am starting to be really desperate and just can't find any reason why.\n",
      "Hi guys! I want to develop an image database for machine learning, but not sure about relational (like SQL) or non relational(NoSQL) database, any thoughts? which one do you recommend me? the main idea is to do some processing on the images and finally sending into an app (probably Flutter)\n",
      "Hi! does anyone know of a good tracker (object detection + tracking in frame sequence) that uses a few shot classifier? I wanted to use deep sort with Yolov3 but my problem is that I have very few images for most of my classes, I don't think Yolo would work very well with such skeewed classes would it? I'm trying to track animals in sequences of photos taken with trap cameras in the wild, I have to use a limited dataset (I can't just google more images of the animals) and there's very few (<5) pictures of some of the animals, any suggestions?\n",
      "How long would it take to train an RNN on 200MB of text? is 200MB too much to handle for google colab?\n",
      "How would I format a dataset for hot word detection?\n",
      "I am new to ML and I have a basic general idea. I wanted to learn more about GANs and so I started with a book and took a simple DCGAN example. At first, I run the program provided by the book but it didn't work. After several days I found out that I had keras version 2.3.1 wheras the book uses 2.1.6. Once I downgraded back to 2.1.6, everything worked as expected. I am very curious to know what's going on, any ideas?\n",
      "I am wondering if anyone is familiar with neural-style or neural-style-pt? I'm working with it, but getting some severe patchy splotchyness and I'm really hoping someone can help me understand the source of that splotchiness and how to fix it.\n",
      "I have a made a model using keras for OCR. I was wondering now what is the next step to use the model with data outside of the training and testing data. Do I need to make a model to extract features from a picture to find the text so it can pass the input to the model?\n",
      "i have a MNIST handwritten dataset with RGB photos of size (90, 90)\n",
      "I want a convnet to recognize when I say \"Wake up\", and when I say either anything else or nothing/background noise.\n",
      "If lets say the load was 700KWH for the last 20 minutes and the temperature is roughly around 40 degrees i would want the forecast to stay around 40\n",
      "if the temperature would have dropped lets say at t-5 to 200 i would like to forecast the temperature fall if the load would stay the same for my forecast\n",
      "In NLP, how are two similar sentences \"grouped\" together, I can't remember which ML algorithm it was to measure their similarity. e.g. calculating similarity scores for “I like to tailgate when I go to the stadium” and “I like running when I go to the stadium”?\n",
      "its my first approach to fc ts, and i surely doing first timers errors\n",
      "Model is MobileNetV3.\n",
      "Need some help here: For image classification problems like cat vs dogs, the output layer is 2. Image classification problems like diabetic retinopathy seem to be more of a grading classifier. Although the targets range from 0 to 4, (signifying the severity of the condition), is it better to have 1 as the output layer, or 5, for these kinds of problems? I have seen Kaggle kernels where both are utilized.\n",
      "One question In paper \"Alleviating catastrophic forgetting using context-dependent gating and synaptic stabilization\", it has been proposed a method named XdG. I think the idea is like dropout, right? If yes, what is the purpose of this paper? It is not new method or even idea in 2019! It has a beautiful name \"context-dependent gating\" but so..(edited)\n",
      "right now I've tried random slices of data from continuous streams of me saying either the hot word or not, but the false positives and false negatives are waaaaaaaaaaay too many, and the architecture is more or less this one but with a sigmoid instead of a softmax and less feature channels https://pytorch.org/tutorials/intermediate/speech_command_recognition_with_torchaudio.html#define-the-network\n",
      "Something pretty like this one from the ResNet paper\n",
      "train_generator = train_datagen.flow_from_directory( file_train, target_size=(256, 256), batch_size=16, class_mode='binary') is running slow for me, currently my files are stored in google drive, is there a faster way for me to store/load my files? I'm guessing I should just render them all to what my generator does, to reduce time doing image processing?\n",
      "What kind of model is good for a task such as finding the optimal input matrix that when fed into a black box program gives the highest metric? The black box isn't random, but we don't know how it gives scores.\n",
      "¡Hola! I'm Heriberto, from México.\n",
      "Hei anuj, It seems we have same background. Can you tell me the application data science project in manufacturing field? I am very interested in it.\n",
      "Hello Aniket from India\n",
      "Hello everyone ! I'm Kawtare, I'm from France, I study psychology and artificial intelligence ! I'm learning deep learning, machine learning ...\n",
      "hello everyone i am pavan mahajan from india\n",
      "Hello everyone I am Vineesh from India. I am working as project engineer in Oil and Gas industry. I am looking to transition into data science field and I have started working on it. Here to gain more insights and knowledge.\n",
      "hello everyone im a SME in statistics at chegg and also working as a data analyst at netomi\n",
      "Hello Everyone, This is Abdurahman. I have a bachelor's degree in Software Engineering. I am currently doing my master's degree in Machine learning. I'm looking forward to sharing ideas and help each other with fellow scientists.\n",
      "Hello guy Big G from France\n",
      "Hello, I am a compsci student gathering knowledge about AI and data science for future studies.\n",
      "Hello, I'm Jon from the US\n",
      "Hello, I'm yuki and I am trying to learn ML on my own, hopefully will grow with your support!\n",
      "Hello, Im Rohan and I live in NY and I'm interested in ML(edited)\n",
      "Hello, My name is Mert. I am Freshman CS student from Turkey. I have passion for Data Science and ML Engineering. I am improving myself in those areas\n",
      "Hello! I'm Artur from Brazil\n",
      "Hello! My name is Christy, I am a recently graduated MSDS from Utica college and I currently work as an analyst in higher education. I have just started using Dataiku as a ML platform which has been quite interesting and complicated. I’m still new to all things machine learning, although I do have some skills and knowledge in their use, strengths and weakness.\n",
      "Hey everyone. I'm just a hobbyist who's interested in creating entertaining content using machine learning.\n",
      "Hi Ariel from Bolivia\n",
      "Hi everyone, I am Kelvin from Nairobi, Kenya. Happy to be here, looking forward to learning a whole lot and sharing ideas with all of you.\n",
      "Hi everyone, I am Mayowa. A Data Science student looking for knowledge,experience and maybe internships\n",
      "Hi everyone, i'm beginner python programmer and i am interested in ML. Looking forward to meeting you guys!(edited)\n",
      "Hi everyone, I'm Claudio and I'm in the process of graduating from an artificial intelligence Master degree. I'm looking for new insights and sources to keep up with the fastly-changing state of the art\n",
      "Hi everyone. My name is Carter. I'm a solution analyst with Deloitte, and I do consulting work for gov't clients to solve analytics and data engineering problems. I primarily use R, Python, and PowerBI. I'm also a GCP-certified Data Engineer and am working on my AWS experience. I would like to serve as a resource for anyone who'd like help with R/RStudio, especially those who are working through R4DS or learning about the tidyverse. Please feel free to reach out and connect through Discord or LinkedIn and I'd love to network with you.\n",
      "Hi, everyone, I am beginning to start learning machine learning. I need hints from where I can start learning. I know few algorithms like vanilla neural networks and linear regression\n",
      "Hi, I'm Lena from Malaysia! I'm an actuarial science graduate who's trying to branch into ML. Nice to meet everyone!\n",
      "Hi, Michael from Poland. Nice to meet you folks.\n",
      "hiya this is Vivek from London, want to delve into AI/ML\n",
      "I know nothing of anything about AI right now but something I do want to look into is deep learning were the idea is the robot simply gets smarter long i keep it on.\n",
      "Is it from Andrew Ng? I have enrolled it. Do you mind give me another resource? Honestly i need project based learning resource.\n",
      "Hello everyone, I am Quinten and I studied Physics and astronomy. After my studies I rolled into a data science carreer at a law firm. I joined to get in touch with fellow data oriented people :)\n",
      "Hi my name is Quincy. I recently graduated with a bachelor degree in computer science. I am currently taking on a course on data science and would like to have conversations with like minded people around that. I hope we get along and grow together\n",
      "Hello everyone, I am Vishwas from India. I work as a Data Scientist in a Pharma company (ACG) working on problems related to IIOT applications in creating solutions related to predictive maintenance, anomaly detection and other machine learning topics related to IOT based applications.\n",
      "Greeting everyone, this is Abdullah Khan from India... I recently completed my undergrad in commerce. I wanna transition my career into data science n I'm here to learn the same.\n",
      "Hello all. I'm just a hobbyist without a hobby. I just order a NVIDIA Jetson Nano (4G B01) and am very interested in learning all about AI. Odd but my love into this all started from seeing the Vector Robot. I want to see if I can build one which of course is a bit smarter. My hopes is to build a home robot which can act as a mouse and find stuff and take it back home. The base idea anyway.(edited)\n",
      "thanks for your advice, Sora\n",
      "10 Amazing Python Hacks with Cool Libraries - datamahadev.com In this article, we will learn some amazing python hacks with some rare yet cool libraries. The main purpose of this article is to learn(or automate) a few basic things with the help of python. So, let us begin.\n",
      "10 python hacks you must know : https://datamahadev.com/10-amazing-python-hacks-with-cool-libraries/ datamahadev.com\n",
      "A good resource for inverse transformation sampling for those who need it.\n",
      "Chatbots have always been so fascinating to me. I was always curious to study about chatbots. Today, I am happy to share that my first try at \"building a Retrieval based Chatbot\" is successful. Oh yes! I created my first chatbot. Feel free to share your feedback. You can find the code and complete documentation for this chatbot project from the link below. https://datamahadev.com/building-a-chatbot-in-python/\n",
      "Creating a Custom R Package I don’t know about you, but I find the method that R uses to sort data frames incredibly non-intuitive. Every time I wanted to sort a data…\n",
      "Finished my article on the scalping market for Big Navi/RDNA2 GPUs. It is far, far smaller than the RTX 30 series market. https://dev.to/driscoll42/big-navi-rdna2-series-scalping-market-analysis-2c3k Summary: * 3,461 RDNA2 GPUs sold on eBay/StockX * $3.95 million in sales on eBay/StockX for ~$944k in profit for scalpers and $615k for eBay/PayPal/StockX * 6800 XT price increased 17% since 1/1 while 6900 and 6800 only increased 7% * RX 5000 and Vega cards have increased 33% or more since 1/1\n",
      "Have you ever thought of building your chatbot? If you have, then this project is going to be your first step in the innovative field of chatbots.\n",
      "hello everyone, you can check out my article about scikit learn design. https://burcukoculu.medium.com/scikit-learn-design-with-easy-explanation-b3bcb060580\n",
      "Hello Everyone. I have written a blog on image similarity without using any fancy techniques such as CNNs and more. It uses pure maths and basic programming and the results are pretty good. Please do give it a read. https://www.analyticsvidhya.com/blog/2021/03/a-beginners-guide-to-image-similarity-using-python/ I am also new to this group but I aim to learn a lot from you guys. Thank you everyone\n",
      "hey everyone , recently i published a blog on Survival Functions, would appreciate if you guys can check it out https://medium.com/p/survival-functions-101-bd57fb8be888?source=email-2d5085b9e12f--writer.postDistributed&sk=74d03f5707b3b48f5d8a4c7a07299b85\n",
      "Hey everyone, I just finished the first chunk of this project: https://github.com/CharlesAverill/satyrn, an alternative to jupyter notebooks that allows you to run cells in parallel. It’s especially useful in the data science field as you can process unrelated data in parallel, often greatly speeding up total runtime. However there’s no UI yet, it’s completely command-line-based. If anyone wants to join the project and help build the frontend, I would greatly appreciate it. Thank you!\n",
      "Hey everyone! A bunch of us got fed up with how rigid the PyTorch dataloader can be at times. We ended up building some scaffolding around the DataLoader class, to enable things like dynamic labels (we use a JSON file to label images, rather than subfolders in a dataset), loading a subset of a dataset, being able to pre-process an image using a custom function (do we want to generate a bunch of crops per source image before loading?) and so on. We're calling it BetterLoader, and I would love to hear what you all think and whether you think a project like this would be helpful. We're finding it useful within our deep learning workflows, and would appreciate a star if you think it would be too  https://github.com/binitai/betterloader(edited)\n",
      "Hey everyone! This is a project of mine that I have been working on. It is a video captioning project. This encoder decoder architecture is used to generate captions describing scene of a video at a particular event. Here is a demo of it working in real time. Check out my Github https://shreyz-max.github.io/Video-Captioning/. Thanks!\n",
      "Hey, everyone! My professor and I developed a simulator for swarm robots in C/C++ and had a fair bit of success. Now I am thinking of implementing a buddy system for the robots where the 2 robots will work together within the swarm and would be considered as a single entity. I am looking for someone who would be interested in mentoring me towards my objective\n",
      "Hi  I wrote a short article introducing \"8 Simple Techniques to Prevent Overfitting\" in Machine Learning for beginners. Hope a few of you may find the toolbox of methods to battle overfitting helpful  https://towardsdatascience.com/8-simple-techniques-to-prevent-overfitting-4d443da2ef7d\n",
      "Hi all, I just published an article via Towards Data Science on an intriguing CVPR 2020 paper, feel free to check it out! https://towardsdatascience.com/oops-predicting-unintentional-action-in-video-87626aab3da3?source=social.tw\n",
      "https://github.com/doccano/doccano - open source text annotation\n",
      "https://myrtle.ai/how-to-train-your-resnet-8-bag-of-tricks/(edited)\n",
      "https://pypi.org/project/satyrn-python/0.8.5/ A jupyter alternative I wrote that supports backend multithreading and branching code, along with network co-editing.\n",
      "https://towardsdatascience.com/the-viola-jones-algorithm-7357c07d8356 Hello everyone I wrote a blog on Viola Jones algorithm which is a foundation for many applications like Object detection, Face detection and many more, your feedback would be highly appreciated!\n",
      "https://www.scratchapixel.com/lessons/mathematics-physics-for-computer-graphics/monte-carlo-methods-mathematical-foundations/inverse-transform-sampling-method\n",
      "I created an awesome list of Discord communities related to everything programming https://github.com/mhxion/awesome-programming-discord#machine-learning The link takes you to the machine learning section. Hope this resource is helpful. Contributions or any feedback welcome.\n",
      "I've just written my first article on Medium, detailing the basics of creating a package in R. I'd appreciate any feedback. https://dikbrown.medium.com/creating-a-custom-r-package-9a2e303d3332\n",
      "Image segmentation is like an advanced form of classification. In Classification, we used to classify pictures into classes. In the case of image segmentation, we classify each pixel of the image into different classes Tutorial to start Image Segmentation using transfer learning using Labelme dataset.\n",
      "Image Similarity | Implement Image Similarity in Python Image similarity is topic not talked about in the field of computer vision. In this article, let us understand Image similarity in Python\n",
      "Introducing pyradox: a python library that helps you with implementing various state of the art neural networks in a totally customizable fashion using TensorFlow 2. You can find pyradox here: https://github.com/Ritvik19/pyradox Check out how it can be used to Detect location of Keypoints on face images: https://ritvik19.medium.com/facial-key-points-detection-using-pyradox-f2a6cf6a862f Don't forget to  the repo!\n",
      "Introducing vizard: a python library that helps you create Intuitive, Easy and Quick Visualizations for Data Science Projects. You can find vizard here: https://github.com/Ritvik19/vizard Check out how it can be used in your data science projects Don't forget to  the repo!\n",
      "Just wanted share my notes, in case they're useful to anyone: https://github.com/jinayshah7/AAIC-Notes I only have handful of them right now, but I'm adding new ones everyday. Encoder-Decoders, Attention, Transformers, BERT, Image Segmentation, etc. Just to name a few.\n",
      "Line art with GAN. https://twitter.com/Vijish68859437/status/1319185184739061760?s=19\n",
      "Nlp repo contain all my current progress from visualisations to custom attentions layers https://github.com/evilc3/NLP_V2 #nlp\n",
      "python notebook tutoriel on kaggle for EDA https://www.kaggle.com/hoodkaggle/eda-pyplot-seaborn-gold-diggerdata\n",
      "Reformer, which was proposed in the paper Reformer: The Efficient Transformer by Google Research, essentially addresses some efficiency constraints of the Transformer model and proposes an improved version of the Transformer that implements Locality Sensitive Hashing (LSH) and Reversible Layers to make the model much more efficient. I’ve made a kaggle kernel implementing the Reformer Model for Named Entity Recognition(NER). Please review the notebook and tell suggestions. https://www.kaggle.com/sauravmaheshkar/trax-ner-using-reformer\n",
      "Samiksha Bhavsar\n",
      "Scikit-Learn Design with Easy Explanation I think we all can agree that Scikit-learn is the main machine learning library for Python. It’s actually beginner friendly library…\n",
      "So i made a deep learning project and even made a video for it https://youtu.be/e4w8W9l1MBk here is the source: https://github.com/fvviz/Digit-recognizer\n",
      "So im finally done with this project i was working on. It lets you play Rock Paper scissor with your computer.Feedback would be awesome video : https://youtu.be/41Q9lkhiZ9Y source code : https://github.com/fvviz/Rock-paper-scissor\n",
      "Today I published an article in DataBulls. Hop onto the link if you want to learn the basic concepts of Neural Networks DataBulls are a growing team of individuals striving to present meaningful outcomes with the help of data. If you're interested to become a part of this community, check out the link below to apply for authorship. https://medium.com/data-myths-and-facts/overview-of-neural-networks-84382d068d78 https://medium.com/data-myths-and-facts/come-write-with-us-as-a-dataprofessional-or-enthusiast-771a641c4a48\n",
      "Toon-Me, Toon Portraits. GitHub link: https://github.com/vijishmadhavan/Toon-Me\n",
      "Top 6 AutoML Frameworks - datamahadev.com In this era of automation, why couldn’t you automate these machine learning pipelines to save time and effort? This is where AutoML frameworks come into the picture. This article will introduce you to the 6 most popular AutoML frameworks.\n",
      "i ve been trying to do a 2d NN (global bathymetry) regression using Tensorflow for a while now.(edited)\n",
      "you should share some results when you're happy with the model. i'm curious to see how you approach that problem\n",
      "you're really big on hydration, eh?\n",
      "that's hella chill\n",
      "@BeanLearning yeah  how do you know that\n",
      "Thats cool\n",
      "Open a command prompt as administrator\n",
      "oh\n",
      "\"hey buddy, some stranger on the internet says they love you\"\n",
      "@Poppa isn't there a Finnish saying about, \"if a sauna, alcohol, or pine pitch won't make you feel better, the doctor can't either?\"\n",
      "Ah Matlab. I remember when I thought it was the shit all the way until my 3rd/4th year of undergrad. Then I found Python and havent looked back\n",
      "also bean how is the problem goin?\n",
      "and hug your pupper?\n",
      "any breakthrough?\n",
      "bahaha\n",
      "because i hate myself, i decided to try MATLAB's neural net architecture on a problem in my CV class (which is being taught in MATLAB)(edited)\n",
      "cons - it is a pain in the butt to debug and there's some nasty stuff hard-coded in\n",
      "damn covid\n",
      "does cool stuff with old rocks\n",
      "doesn't work\n",
      "for my dad\n",
      "for real\n",
      "haha, i have a good friend who's Finnish\n",
      "Has anyone over here used sagemaker? Are there standard values for CPU/ memory utilization in sagemaker? Like say if the % of CPU utilization exceeds x% then it is critical. So is there some standard or average value for x here?\n",
      "he appreciates it  It has everything I want in life.\n",
      "he doesn't use linux\n",
      "holy cow, i'm reading about CNN pre-training with random labels and it's really cool that that can actually be beneficial\n",
      "i hate windows\n",
      "i hope a Microsoft engineer is not online\n",
      "I learned a lot in the process.\n",
      "i really don't know how to take that\n",
      "I wonder if my interviewers also feel the same about me sometimes...\n",
      "It's also available on Linux though no one uses it\n",
      "it's also MATLAB\n",
      "it's cold and wet out and I wish the public sauna was still open here\n",
      "It's like bash but for Windows\n",
      "LMAO\n",
      "lol\n",
      "lol i cant make him\n",
      "lol, is that the formal way of saying \"you thirsty?\"\n",
      "Make him\n",
      "maybe make sure you have a healthy snack and drink lots of water?\n",
      "my main os is Ubuntu\n",
      "nevermind the nice things i previously said, this is hot trash(edited)\n",
      "oh nice\n",
      "once upon a time there was a lovely little pupper called Snoopy and he said if you don't drink a glass of water I will be very sad please drink some and forward this message to 19 people or you will be struck with bad luck for 7 years\n",
      "Open a command prompt as admin\n",
      "or: Winkey-X, \"Windows PowerShell (Admin)\"\n",
      "Photos.\n",
      "pros - it interfaces well with the GPU and is more full featured than expected\n",
      "right click cmd click run as administrator\n",
      "Right!? shit's dope\n",
      "showmeyourdog\n",
      "shOWUSYOURDOG.\n",
      "still trying to decipher this\n",
      "stuff is going, i'm switching advisors so now i'm on a plan of finish the main project to get a journal paper and document the rest so that the next grad student can take it over without too much suffering\n",
      "TELL YOUR PUPPY I LOVE IT.\n",
      "what even is powershell\n",
      "Why do people apply for a developer role, when they don't know how to code? Or why do they even say they know Python when they are unable to make a very simple script using Python\n",
      "yeah, she's a geoscience prof\n",
      "YOU HAVE A DOG!??\n",
      "(please ping me if you reply, thank you)\n",
      "@Sora @BeanLearning I am preparing survey documents for presentation, Latex is new for me, can u suggest software tool for Latex\n",
      "any books that extensively describe stochastic processes as they relate to ml?\n",
      "Anyone got any good intros to reinforcement learning? Currently reading Richard S. Sutton and Andrew G. Bartos book but just wondering if theres any other good resources people would recommend\n",
      "anyone got resources on training models on cloud?\n",
      "anyone know of any good reference implementations for GPU-sided non-max-suppression?\n",
      "Blei's lectures seem legit but no lecture videos available\n",
      "Does anyone have a database of faces labeled with bounded boxes? Trying to get a face tracker project off the ground.\n",
      "Does anyone have a good book on ml with python?\n",
      "does anyone have a resource that teaches convolutional neural nets\n",
      "Does anyone know a resource that explains each topic while explaining all the math involved?\n",
      "Does somebody know a good free book with a good chapter on Q-learning? i need to write a chapter about q-learning and cant find any good resources\n",
      "Good ressources to learn C++ ?\n",
      "Hello All. I have made way through week 7 of Andre Ng's ML course but stopped there. So I am restarting it again from week 1. What really gets me or demotivates me is the Math that is involved. Can anybody tell me what they brushed through and any resources I can go through to get up and running?\n",
      "Hello everybody. I am looking for a deep learning book or free course that doesn't assume any math background beyond high school math. I understand the mathematics behind fully connected layers pretty well (completed the first one and a half courses in Andrew NG's deep learning specialization) so I am looking for a book to expand into convolutional or recurrent networks, so that I can build my own without any libraries. Are there any resources or books that you recommend?\n",
      "Hello Everyone, i know the ML basic, thanks to Andrew NG, can anyone guide me how to proceed further\n",
      "Hello everyone. Probably a very asked question around here. I'm in doubt between two courses. Andrew Ng's Machine Learning @ Coursera and Machine Learning with Python-From Linear Models to Deep Learning @ edX.\n",
      "Hello, I'm looking for a good video tutorial for Numpy / Pandas / Matplotlib. Any of these three would work, Thanks\n",
      "Hey guys, I'm just starting with NLP. Do you recommend any books or videos or any other resources?\n",
      "hi\n",
      "Hi .... Can anyone point me to a few case studies where NLP is used for a product in a B2B context?\n",
      "Hi guys, do you have any engineering focused resources? I love watching talks, and while articles and course are mostly about building a model and testing it, there are really few resources about how to have a nice workflow and pipeline to put that model in production, how to assess its performance in production and, how to improve and iterate on it after you've have your first shot\n",
      "Hi guys, do you know any website/app where we could practice AI skills by making bots to fight each other in a competitive way\n",
      "Hi, any beginner resources for image classification using CNN? Thanks :3\n",
      "Hi. Any hands-on keras course that actually teaches how to code in Keras? Most ML courses are slow af and are for total beginners. I want something that focuses more on Keras API than Deep Learning fundamentals.\n",
      "i swear i've read about them for hours from different sources and still have absolutely no idea what's going on\n",
      "is there a book that’s as visually appealing as d2l.ai but about classical ml methods? that’s odd criteria but i usually lose focus trying to read black and white pdfs\n",
      "Is there any courses or resources for ML where I can learn by making projects?\n",
      "is there some git repo or textbook etc that succinctly lists the pros and cons of common models? whenever i learn about a new model its hard to understand why it would or would not be used\n",
      "Suggest me a good deep learning course on Coursera beside deeplearning.ai\n",
      "what are good resources for learning nlp?\n",
      "Where might I find the answers to the math/written assignments of CS224N? I was able to find a number of GitHub repositories containing answers to the coding exercises, but none to verify my math answers with.\n",
      "What do you guys think of Machine Learning: An Algorithmic Perspective 2nd Edition. I'm planning on buying it, is it worth it or should I purchase another book? Plus I'd love book suggestions for ML/DL. Cheers.\n",
      "Hi everyone! Is there a YT channel that covers news and events in the Artificial intelligence field and community?\n",
      "Any resource to learn Data Warehouse concepts with examples?\n",
      "1st year Msc in Data Science student. resume.pdf\n",
      "Agree with this. If you have a STEM degree, especially an engineering one, that won't disqualify you from CS roles. Philosophy majors will have a more difficult time getting in the door though to any job tbh\n",
      "any tips?\n",
      "Do you think interviewers want to see my code wars profile?\n",
      "Faraz_Khan_-_One_page2.pdf\n",
      "Feel free to update your resume and repost it if you want us to take another pass. In general you have a wonderful resume that you should feel very proud of\n",
      "for example \"i successfully wrote 400 lines of code to solve xyz problem\"\n",
      "Great resume Papa! I love that you started with a personal statement and prominently showcase your skills. Here are a few things I would recommend changing: 1. I assume your name and contact info is normally appended and you just redacted that for this post. If not, I would definitely add it  2. It's not clear to me if you have just a bachelors or a masters as well based on the two GPAs listed. If you have a masters, I would list that as a separate degree from the same school. Right now, its very easy to miss. 3. You don't need to list your 3.1 GPA if you don't want to 4. Your personal statement looks more like a list of projects that a statement about yourself. I would recommend you create a list of projects and list those if you have them. I would write something personal for your personal statement along the lines of what you care about and what you're excited by. 5. If you create a list of projects, make sure you explain the projects you completed. Faraz's resume below has some good examples of how to list projects. 6. A lot of what you list in your personal statement can be put in your skills section, things like data visualization, data cleaning, automation, data analysis can all be skills you put in your skills section. 7. I have a hard time understanding your certifications and extra courses section. It could use further clarification to show what is a certification, what is an extra course, and what it means that you took extra courses. Maybe touch on that in your personal statement about going above and beyond in your education. I think these should be some good tips for now. Feel free to respost your resume with edits and we'll take a second pass at it\n",
      "I have certain question along with these: 1. Do I need to include my club activities that I did in college? 2. Do I need to provide my marks for the degrees? 3. Should I make some important texts in project section bold?\n",
      "I think that for a student is good enough, for the questions 1) I wouldn't 2) If they're good 3) yep, try to underline some keyword like machine learning, python and so on\n",
      "I think these should be some good tips for now. Feel free to respost your resume with edits and we'll take a second pass at it\n",
      "I think this advice may be out of date. ATS these days will convert everything to plaintext which once it reaches a human eye will be obvious what you're doing.\n",
      "If anyone feels uncomfortable sharing their resume publicly, feel free to message any of the experts privately, and they can share your resume anonymously with your personal information removed. That is the case with the following resume which I will give suggestions on.\n",
      "If we're game for some resume-adjacent content, I organized an application I submitted earlier today into a google doc\n",
      "is it a good idea to put your individual (or best 3) coding projects on your resume?(edited)\n",
      "of course that happens - that's why judging people by their college never helps. its just that because of the highly selective process, you are exposed to more smarter and like-minded people that may help you achieve more in research and competetions.\n",
      "Ok cool makes sense. Only list your major gpa but you should say major because i didn’t understand it otherwise\n",
      "Papa Kamis — 03/28/2021\n",
      "something for your \"other\" section at best if you're a very fresh grad or an undergrad seeking an internship but probably they aren't gonna click on it anyway\n",
      "Sure. Im a uni student, but yet i do self learning for each and every practical skill. Sigh. Guided learning is great, but if you do self learning along with it, that's a cherry on top.\n",
      "Thanks @antonai , will underline the keywords\n",
      "Thanks for sharing @frazali52 i will get to this tonight\n",
      "Thanks for sharing your resume Faraz. It looks excellent. You have great formatting, I love that you bolded the key terms and figures, and I like that you led with technical skills. Improving your resume required getting a little more creative, but here are my ideas: 1. You may benefit from a personal statement. Personal statements are an opportunity to bring some humanity and a face to your resume, which is otherwise fairly flat. Just like product ads don't just list the features, but try and connect to the customer, you might benefit from a personal statement that resonates with a potential employer. Examples might be: you always go the extra mile to exceed client expectations, you love storytelling and data is a way you love to make your stories better, or youre entrepreneurial and you try new solutions. 2. Another way to bring a personal touch is to add your hobbies. Most employers want to work with people they'll also like being around, and I'm sure you're a great person to be around. If you're afraid that sharing your hobbies might be too foreign or unfamiliar to your employer, I have very unique hobbies and interests on my resume, like Talmud. Usually it's just a conversation starter. 3. You have work experience but you don't have big brand names on your resume to give you credibility (besides for your university). Since you've done a lot of projects for a lot of different people, you may benefit from a section that lists references from people you've worked for, and even reviews from those people if you have them. If you have one or two glowing reviews on your resume, itd be hard for anyone to discount that. And definitely give a brief background of the people you're providing as references if you do. 4. You have a great skills section, but you could always add more. Often the goal is hit on an employer search term, so don't be bashful about adding in additional terms that you think your employer may be searching for. Like data cleaning for example\n",
      "yes of course, just delete any address, phone number, etc.\n",
      "Yes, but be prepared to speak intelligently to anything you put on your resume\n"
     ]
    }
   ],
   "source": [
    "# Check stemming \n",
    "df_word_cleaned = []\n",
    "\n",
    "for i in df_raw['Utterance']:\n",
    "    wordtokenList = [word for word in word_tokenize(new_sent)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check Lemmatisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanText(text):\n",
    "    df_word_cleaned = []\n",
    "    for i in range(len(text)):\n",
    "        new_sent = re.sub(r'\\d+', '', text.iloc[i]).lower() #remove numbers\n",
    "        new_sent = re.sub(r'http\\S+', '', new_sent) # remove url\n",
    "        wordtokenList = [word for word in word_tokenize(new_sent)]\n",
    "        wordNoStopList = [word for word in wordtokenList if word not in customStopWords]\n",
    "        lemList = [wnl.lemmatize(word) for word in wordNoStopList]\n",
    "        df_word_cleaned.append([porter_stemmer.stem(word) for word in lemList])\n",
    "    # re-assemble the token back to sentence form \n",
    "    cleaned_sent = []\n",
    "    for sentence in df_word_cleaned:\n",
    "        cleaned_sent.append(TreebankWordDetokenizer().detokenize(sentence))\n",
    "    cleaned_sent\n",
    "    \n",
    "    return cleaned_sent\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get TF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer()\n",
    "tf = cv.fit_transform(cleaned_sent)\n",
    "df_tf = pd.DataFrame.sparse.from_spmatrix(tf, columns= cv.get_feature_names())#.to_csv(\"tf.csv\")\n",
    "df_tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>10</th>\n",
       "      <th>2021</th>\n",
       "      <th>able</th>\n",
       "      <th>able to</th>\n",
       "      <th>about</th>\n",
       "      <th>about ai</th>\n",
       "      <th>actually</th>\n",
       "      <th>add</th>\n",
       "      <th>adding</th>\n",
       "      <th>advanced</th>\n",
       "      <th>...</th>\n",
       "      <th>you guys</th>\n",
       "      <th>you have</th>\n",
       "      <th>you re</th>\n",
       "      <th>you recommend</th>\n",
       "      <th>you should</th>\n",
       "      <th>you think</th>\n",
       "      <th>you to</th>\n",
       "      <th>you want</th>\n",
       "      <th>your</th>\n",
       "      <th>your resume</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.347712</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>295</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>296</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>297</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.026514</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.078043</td>\n",
       "      <td>0.04224</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.167594</td>\n",
       "      <td>0.107408</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.040469</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.304612</td>\n",
       "      <td>0.183688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>298</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.231913</td>\n",
       "      <td>0.307668</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>300 rows × 756 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      10  2021  able  able to     about  about ai  actually       add  \\\n",
       "0    0.0   0.0   0.0      0.0  0.000000       0.0       0.0  0.000000   \n",
       "1    0.0   0.0   0.0      0.0  0.000000       0.0       0.0  0.000000   \n",
       "2    0.0   0.0   0.0      0.0  0.347712       0.0       0.0  0.000000   \n",
       "3    0.0   0.0   0.0      0.0  0.000000       0.0       0.0  0.000000   \n",
       "4    0.0   0.0   0.0      0.0  0.000000       0.0       0.0  0.000000   \n",
       "..   ...   ...   ...      ...       ...       ...       ...       ...   \n",
       "295  0.0   0.0   0.0      0.0  0.000000       0.0       0.0  0.000000   \n",
       "296  0.0   0.0   0.0      0.0  0.000000       0.0       0.0  0.000000   \n",
       "297  0.0   0.0   0.0      0.0  0.026514       0.0       0.0  0.078043   \n",
       "298  0.0   0.0   0.0      0.0  0.000000       0.0       0.0  0.000000   \n",
       "299  0.0   0.0   0.0      0.0  0.000000       0.0       0.0  0.000000   \n",
       "\n",
       "      adding  advanced  ...  you guys  you have    you re  you recommend  \\\n",
       "0    0.00000       0.0  ...       0.0  0.000000  0.000000            0.0   \n",
       "1    0.00000       0.0  ...       0.0  0.000000  0.000000            0.0   \n",
       "2    0.00000       0.0  ...       0.0  0.000000  0.000000            0.0   \n",
       "3    0.00000       0.0  ...       0.0  0.000000  0.000000            0.0   \n",
       "4    0.00000       0.0  ...       0.0  0.000000  0.000000            0.0   \n",
       "..       ...       ...  ...       ...       ...       ...            ...   \n",
       "295  0.00000       0.0  ...       0.0  0.000000  0.000000            0.0   \n",
       "296  0.00000       0.0  ...       0.0  0.000000  0.000000            0.0   \n",
       "297  0.04224       0.0  ...       0.0  0.167594  0.107408            0.0   \n",
       "298  0.00000       0.0  ...       0.0  0.000000  0.000000            0.0   \n",
       "299  0.00000       0.0  ...       0.0  0.000000  0.000000            0.0   \n",
       "\n",
       "     you should  you think  you to  you want      your  your resume  \n",
       "0           0.0   0.000000     0.0       0.0  0.000000     0.000000  \n",
       "1           0.0   0.000000     0.0       0.0  0.000000     0.000000  \n",
       "2           0.0   0.000000     0.0       0.0  0.000000     0.000000  \n",
       "3           0.0   0.000000     0.0       0.0  0.000000     0.000000  \n",
       "4           0.0   0.000000     0.0       0.0  0.000000     0.000000  \n",
       "..          ...        ...     ...       ...       ...          ...  \n",
       "295         0.0   0.000000     0.0       0.0  0.000000     0.000000  \n",
       "296         0.0   0.000000     0.0       0.0  0.000000     0.000000  \n",
       "297         0.0   0.040469     0.0       0.0  0.304612     0.183688  \n",
       "298         0.0   0.000000     0.0       0.0  0.000000     0.000000  \n",
       "299         0.0   0.000000     0.0       0.0  0.231913     0.307668  \n",
       "\n",
       "[300 rows x 756 columns]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer(min_df = 3,ngram_range = (1,3))\n",
    "transformed_documents = vectorizer.fit_transform(df_raw['Utterance'])\n",
    "# transformed_documents = vectorizer.fit_transform(cleanText(df_raw['Utterance']))\n",
    "df_transform_documents = pd.DataFrame.sparse.from_spmatrix(transformed_documents, columns=vectorizer.get_feature_names())#.to_csv('cleanedminTF3ng3.csv')\n",
    "df_transform_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10             0.003264\n",
       "2021           0.004097\n",
       "able           0.001416\n",
       "able to        0.001416\n",
       "about          0.013907\n",
       "                 ...   \n",
       "you think      0.002372\n",
       "you to         0.001870\n",
       "you want       0.002660\n",
       "your           0.018666\n",
       "your resume    0.004428\n",
       "Length: 756, dtype: float64"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This cannot replace 0 with nan due to \"pd.DataFrame.sparse.from_spmatrix\" stored columns as sparse matrix\n",
    "df_transform_documents.mean(axis=0)#.sort_values(ascending=False)[0:60]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "showusyourdog    1.000000\n",
       "lmao             1.000000\n",
       "bahaha           1.000000\n",
       "showmeyourdog    1.000000\n",
       "breakthrough     0.890691\n",
       "                   ...   \n",
       "mile             0.045750\n",
       "creative         0.045750\n",
       "definitely       0.044400\n",
       "looks            0.044400\n",
       "faraz            0.044400\n",
       "Length: 2060, dtype: float64"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_tfidf.replace(0, np.NaN).mean(axis=0).sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer2 = TfidfVectorizer(stop_words='english')\n",
    "transformed_documents2 = vectorizer2.fit_transform(df_raw['Utterance'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "756"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF using cleaned text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(min_df = 3,ngram_range = (1,3))\n",
    "transformed_documents = vectorizer.fit_transform(cleaned_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>abl</th>\n",
       "      <th>actual</th>\n",
       "      <th>ad</th>\n",
       "      <th>add</th>\n",
       "      <th>addit</th>\n",
       "      <th>address</th>\n",
       "      <th>admin</th>\n",
       "      <th>advanc</th>\n",
       "      <th>advic</th>\n",
       "      <th>agre</th>\n",
       "      <th>...</th>\n",
       "      <th>work</th>\n",
       "      <th>would</th>\n",
       "      <th>would like</th>\n",
       "      <th>write</th>\n",
       "      <th>written</th>\n",
       "      <th>wrote</th>\n",
       "      <th>yacov</th>\n",
       "      <th>ye</th>\n",
       "      <th>year</th>\n",
       "      <th>yet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.252394</td>\n",
       "      <td>0.36034</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.291326</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.177472</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>295</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>296</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>297</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.070396</td>\n",
       "      <td>0.135757</td>\n",
       "      <td>0.073478</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.151735</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>298</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.483349</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.463078</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.469502</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>300 rows × 430 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     abl  actual        ad       add     addit   address     admin  advanc  \\\n",
       "0    0.0     0.0  0.000000  0.000000  0.000000  0.000000  0.000000     0.0   \n",
       "1    0.0     0.0  0.000000  0.000000  0.000000  0.000000  0.000000     0.0   \n",
       "2    0.0     0.0  0.000000  0.000000  0.000000  0.000000  0.000000     0.0   \n",
       "3    0.0     0.0  0.000000  0.000000  0.000000  0.000000  0.291326     0.0   \n",
       "4    0.0     0.0  0.000000  0.000000  0.000000  0.000000  0.000000     0.0   \n",
       "..   ...     ...       ...       ...       ...       ...       ...     ...   \n",
       "295  0.0     0.0  0.000000  0.000000  0.000000  0.000000  0.000000     0.0   \n",
       "296  0.0     0.0  0.000000  0.000000  0.000000  0.000000  0.000000     0.0   \n",
       "297  0.0     0.0  0.070396  0.135757  0.073478  0.000000  0.000000     0.0   \n",
       "298  0.0     0.0  0.000000  0.000000  0.000000  0.483349  0.000000     0.0   \n",
       "299  0.0     0.0  0.000000  0.000000  0.000000  0.000000  0.000000     0.0   \n",
       "\n",
       "     advic  agre  ...      work     would  would like  write  written  wrote  \\\n",
       "0      0.0   0.0  ...  0.000000  0.000000     0.00000    0.0      0.0    0.0   \n",
       "1      0.0   0.0  ...  0.000000  0.252394     0.36034    0.0      0.0    0.0   \n",
       "2      0.0   0.0  ...  0.000000  0.000000     0.00000    0.0      0.0    0.0   \n",
       "3      0.0   0.0  ...  0.000000  0.177472     0.00000    0.0      0.0    0.0   \n",
       "4      0.0   0.0  ...  0.000000  0.000000     0.00000    0.0      0.0    0.0   \n",
       "..     ...   ...  ...       ...       ...         ...    ...      ...    ...   \n",
       "295    0.0   0.0  ...  0.000000  0.000000     0.00000    0.0      0.0    0.0   \n",
       "296    0.0   0.0  ...  0.000000  0.000000     0.00000    0.0      0.0    0.0   \n",
       "297    0.0   0.0  ...  0.151735  0.000000     0.00000    0.0      0.0    0.0   \n",
       "298    0.0   0.0  ...  0.000000  0.000000     0.00000    0.0      0.0    0.0   \n",
       "299    0.0   0.0  ...  0.000000  0.000000     0.00000    0.0      0.0    0.0   \n",
       "\n",
       "     yacov        ye  year  yet  \n",
       "0      0.0  0.000000   0.0  0.0  \n",
       "1      0.0  0.000000   0.0  0.0  \n",
       "2      0.0  0.000000   0.0  0.0  \n",
       "3      0.0  0.000000   0.0  0.0  \n",
       "4      0.0  0.000000   0.0  0.0  \n",
       "..     ...       ...   ...  ...  \n",
       "295    0.0  0.000000   0.0  0.0  \n",
       "296    0.0  0.000000   0.0  0.0  \n",
       "297    0.0  0.000000   0.0  0.0  \n",
       "298    0.0  0.463078   0.0  0.0  \n",
       "299    0.0  0.469502   0.0  0.0  \n",
       "\n",
       "[300 rows x 430 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_transform_document = pd.DataFrame.sparse.from_spmatrix(transformed_documents,columns=vectorizer.get_feature_names())\n",
    "df_transform_document#.to_csv('Cleaned TFIDF.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    df_raw, df_raw['Channel'], test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(min_df = 3, max_df = 0.6, ngram_range = (1,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = SVC()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_documents = vectorizer.fit_transform(X_train['Utterance'])\n",
    "df_transform_documents = pd.DataFrame.sparse.from_spmatrix(transformed_documents,columns=vectorizer.get_feature_names())\n",
    "\n",
    "scores = cross_val_score(clf, df_transform_documents.values, y_train, cv=10, n_jobs=-1,scoring='accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.51851852, 0.55555556, 0.66666667, 0.59259259, 0.48148148,\n",
       "       0.59259259, 0.55555556, 0.55555556, 0.59259259, 0.48148148])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<270x708 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 5680 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformed_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainning 0.9962962962962963\n"
     ]
    }
   ],
   "source": [
    "clf.fit(transformed_documents,y_train)\n",
    "print(\"Trainning\", clf.score(transformed_documents,y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_transfomend = vectorizer.transform(X_test['Utterance'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5666666666666667\n"
     ]
    }
   ],
   "source": [
    "print(clf.score(test_transfomend,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<sklearn.metrics._plot.confusion_matrix.ConfusionMatrixDisplay at 0x187969cd828>"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_confusion_matrix(clf, test_transfomend, y_test, cmap=plt.cm.Blues)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test cleaned tf-idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    df_raw['Utterance'], df_raw['Channel'], test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "232                                       shOWUSYOURDOG.\n",
       "59     Please keep this channel only for information ...\n",
       "6      FAQ Update We've updated the Frequently Asked ...\n",
       "185               Open a command prompt as administrator\n",
       "173    Scikit-Learn Design with Easy Explanation I th...\n",
       "                             ...                        \n",
       "188    @Poppa isn't there a Finnish saying about, \"if...\n",
       "71              does anyone has any experience with mds?\n",
       "106    Hei anuj, It seems we have same background. Ca...\n",
       "270            what are good resources for learning nlp?\n",
       "102    Something pretty like this one from the ResNet...\n",
       "Name: Utterance, Length: 240, dtype: object"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train = cleanText(X_train)\n",
    "vectorizer = TfidfVectorizer(min_df = 3, ngram_range = (1,3))\n",
    "transformed_documents = vectorizer.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_transform_document = pd.DataFrame.sparse.from_spmatrix(transformed_documents,columns=vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>10</th>\n",
       "      <th>able</th>\n",
       "      <th>able to</th>\n",
       "      <th>about</th>\n",
       "      <th>about ai</th>\n",
       "      <th>actually</th>\n",
       "      <th>add</th>\n",
       "      <th>after</th>\n",
       "      <th>again</th>\n",
       "      <th>agree</th>\n",
       "      <th>...</th>\n",
       "      <th>you can</th>\n",
       "      <th>you do</th>\n",
       "      <th>you guys</th>\n",
       "      <th>you have</th>\n",
       "      <th>you re</th>\n",
       "      <th>you recommend</th>\n",
       "      <th>you to</th>\n",
       "      <th>you want</th>\n",
       "      <th>your</th>\n",
       "      <th>your resume</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.137601</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.262595</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.274614</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>235</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.290775</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>236</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>237</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>238</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>239</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>240 rows × 584 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      10  able  able to     about  about ai  actually  add  after  again  \\\n",
       "0    0.0   0.0      0.0  0.000000       0.0  0.000000  0.0    0.0    0.0   \n",
       "1    0.0   0.0      0.0  0.000000       0.0  0.000000  0.0    0.0    0.0   \n",
       "2    0.0   0.0      0.0  0.137601       0.0  0.000000  0.0    0.0    0.0   \n",
       "3    0.0   0.0      0.0  0.000000       0.0  0.000000  0.0    0.0    0.0   \n",
       "4    0.0   0.0      0.0  0.000000       0.0  0.262595  0.0    0.0    0.0   \n",
       "..   ...   ...      ...       ...       ...       ...  ...    ...    ...   \n",
       "235  0.0   0.0      0.0  0.290775       0.0  0.000000  0.0    0.0    0.0   \n",
       "236  0.0   0.0      0.0  0.000000       0.0  0.000000  0.0    0.0    0.0   \n",
       "237  0.0   0.0      0.0  0.000000       0.0  0.000000  0.0    0.0    0.0   \n",
       "238  0.0   0.0      0.0  0.000000       0.0  0.000000  0.0    0.0    0.0   \n",
       "239  0.0   0.0      0.0  0.000000       0.0  0.000000  0.0    0.0    0.0   \n",
       "\n",
       "        agree  ...  you can  you do  you guys  you have  you re  \\\n",
       "0    0.000000  ...      0.0     0.0       0.0       0.0     0.0   \n",
       "1    0.000000  ...      0.0     0.0       0.0       0.0     0.0   \n",
       "2    0.000000  ...      0.0     0.0       0.0       0.0     0.0   \n",
       "3    0.000000  ...      0.0     0.0       0.0       0.0     0.0   \n",
       "4    0.274614  ...      0.0     0.0       0.0       0.0     0.0   \n",
       "..        ...  ...      ...     ...       ...       ...     ...   \n",
       "235  0.000000  ...      0.0     0.0       0.0       0.0     0.0   \n",
       "236  0.000000  ...      0.0     0.0       0.0       0.0     0.0   \n",
       "237  0.000000  ...      0.0     0.0       0.0       0.0     0.0   \n",
       "238  0.000000  ...      0.0     0.0       0.0       0.0     0.0   \n",
       "239  0.000000  ...      0.0     0.0       0.0       0.0     0.0   \n",
       "\n",
       "     you recommend  you to  you want  your  your resume  \n",
       "0              0.0     0.0       0.0   0.0          0.0  \n",
       "1              0.0     0.0       0.0   0.0          0.0  \n",
       "2              0.0     0.0       0.0   0.0          0.0  \n",
       "3              0.0     0.0       0.0   0.0          0.0  \n",
       "4              0.0     0.0       0.0   0.0          0.0  \n",
       "..             ...     ...       ...   ...          ...  \n",
       "235            0.0     0.0       0.0   0.0          0.0  \n",
       "236            0.0     0.0       0.0   0.0          0.0  \n",
       "237            0.0     0.0       0.0   0.0          0.0  \n",
       "238            0.0     0.0       0.0   0.0          0.0  \n",
       "239            0.0     0.0       0.0   0.0          0.0  \n",
       "\n",
       "[240 rows x 584 columns]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_transform_document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<30x673 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 444 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_transfomend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = cleanText(X_test)\n",
    "test_transfomend = vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=1)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = SVC(C=1)\n",
    "clf.fit(transformed_documents,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9916666666666667"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.score(transformed_documents,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4166666666666667"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.score(test_transfomend,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<sklearn.metrics._plot.confusion_matrix.ConfusionMatrixDisplay at 0x1780c6531d0>"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_confusion_matrix(clf, test_transfomend, y_test, cmap=plt.cm.Blues)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                          precision    recall  f1-score   support\n",
      "\n",
      "                 General       0.00      0.00      0.00         5\n",
      "            Introduction       1.00      0.50      0.67         4\n",
      "               Off-topic       0.35      1.00      0.52        17\n",
      "               Resources       0.50      0.33      0.40         3\n",
      "           Resume Review       0.00      0.00      0.00         6\n",
      "      community-requests       0.67      0.57      0.62         7\n",
      "       general-questions       1.00      0.09      0.17        11\n",
      "member-generated-content       0.00      0.00      0.00         7\n",
      "\n",
      "                accuracy                           0.42        60\n",
      "               macro avg       0.44      0.31      0.30        60\n",
      "            weighted avg       0.45      0.42      0.32        60\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "print(classification_report(y_test, clf.predict(test_transfomend)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 2,  0,  1,  1,  0,  1,  0,  0],\n",
       "       [ 0,  3,  1,  0,  0,  0,  0,  0],\n",
       "       [ 0,  0, 14,  0,  0,  0,  2,  1],\n",
       "       [ 0,  0,  0,  3,  0,  0,  0,  0],\n",
       "       [ 0,  0,  2,  1,  1,  0,  0,  2],\n",
       "       [ 0,  0,  0,  0,  0,  7,  0,  0],\n",
       "       [ 0,  0,  8,  0,  0,  0,  3,  0],\n",
       "       [ 0,  0,  1,  1,  0,  0,  1,  4]], dtype=int64)"
      ]
     },
     "execution_count": 227,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y_test, clf.predict(test_transfomend)).ravel().reshape([8,8])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline and Utility Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pipeline = Pipeline(steps=[\n",
    "    ('tfidf', TfidfVectorizer()),\n",
    "    ('svc', SVC())\n",
    "])\n",
    "\n",
    "parameters = {\n",
    "#     'preprocessor':[],\n",
    "#     'stop_words':[],\n",
    "    'tfidf__ngram_range':[(1,1),(1,2),(1,3)],\n",
    "    'tfidf__max_df':[1,.9,.8,.7,.6],\n",
    "    'tfidf__min_df':[1,2,3,4,5],\n",
    "#     'max_features':[],\n",
    "    'svc__C':[1,.9,.7,.5,.2,.1],\n",
    "    \n",
    "}\n",
    "\n",
    "# grid search\n",
    "grid_search = GridSearchCV(pipeline, parameters, n_jobs=-1, verbose=1)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    df_raw['Utterance'], df_raw['Channel'], test_size=0.1, random_state=42)\n",
    "\n",
    "grid_search.fit(X_train,y_train)\n",
    "\n",
    "grid_search.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('int32')"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label.dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "le = preprocessing.LabelEncoder()\n",
    "\n",
    "label = le.fit_transform(df_raw['Channel'])\n",
    "# label = label.astype('long')\n",
    "\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "    df_raw['Utterance'], label, test_size=0.4, random_state=412,stratify=label)\n",
    "\n",
    "\n",
    "X_test, X_val, y_test, y_val = train_test_split(\n",
    "    X_temp, y_temp, test_size=0.5, random_state=412,stratify=y_temp)\n",
    "\n",
    "# import BERT-base pretrained model\n",
    "bert = AutoModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Load the BERT tokenizer\n",
    "tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample data\n",
    "text = [\"this is a bert model tutorial\", \"we will fine-tune a bert model\"]\n",
    "\n",
    "# encode text\n",
    "# tokens_sa = tokenizer.batch_encode_plus(text, padding=True, return_token_type_ids=False,max_length=3)\n",
    "tokens_sa = tokenizer.batch_encode_plus(text, padding=True,max_length=3,truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 101, 2023,  102],\n",
       "        [ 101, 2057,  102]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_seq = torch.tensor(tokens_sa['input_ids'])\n",
    "train_mask = torch.tensor(tokens_sa['attention_mask'])\n",
    "train_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# get length of all the messages in the train set\n",
    "seq_len = [len(i.split()) for i in X_train]\n",
    "\n",
    "pd.Series(seq_len).hist(bins = 30)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_seq_len = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize and encode sequences in the training set\n",
    "tokens_train = tokenizer.batch_encode_plus(\n",
    "    X_train.tolist(),\n",
    "    max_length = max_seq_len,\n",
    "    padding=True,\n",
    "    truncation=True,\n",
    "    return_token_type_ids=False\n",
    ")\n",
    "\n",
    "# tokenize and encode sequences in the validation set\n",
    "tokens_val = tokenizer.batch_encode_plus(\n",
    "    X_val.tolist(),\n",
    "    max_length = max_seq_len,\n",
    "    pad_to_max_length=True,\n",
    "    truncation=True,\n",
    "    return_token_type_ids=False\n",
    ")\n",
    "\n",
    "# tokenize and encode sequences in the test set\n",
    "tokens_test = tokenizer.batch_encode_plus(\n",
    "    X_test.tolist(),\n",
    "    max_length = max_seq_len,\n",
    "    padding=True,\n",
    "    truncation=True,\n",
    "    return_token_type_ids=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for train set\n",
    "train_seq = torch.tensor(tokens_train['input_ids'])\n",
    "train_mask = torch.tensor(tokens_train['attention_mask'])\n",
    "train_y = torch.tensor(y_train)\n",
    "\n",
    "# for validation set\n",
    "val_seq = torch.tensor(tokens_val['input_ids'])\n",
    "val_mask = torch.tensor(tokens_val['attention_mask'])\n",
    "val_y = torch.tensor(y_val.tolist())\n",
    "\n",
    "# for test set\n",
    "test_seq = torch.tensor(tokens_test['input_ids'])\n",
    "test_mask = torch.tensor(tokens_test['attention_mask'])\n",
    "test_y = torch.tensor(y_test.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([180, 50])\n",
      "torch.Size([180, 50])\n",
      "torch.Size([180])\n"
     ]
    }
   ],
   "source": [
    "print(train_mask.shape)\n",
    "print(train_seq.shape)\n",
    "print(train_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "#define a batch size\n",
    "batch_size = 32\n",
    "\n",
    "# wrap tensors\n",
    "train_data = TensorDataset(train_seq, train_mask, train_y)\n",
    "\n",
    "# sampler for sampling the data during training\n",
    "train_sampler = RandomSampler(train_data)\n",
    "\n",
    "# dataLoader for train set\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
    "\n",
    "# wrap tensors\n",
    "val_data = TensorDataset(val_seq, val_mask, val_y)\n",
    "\n",
    "# sampler for sampling the data during training\n",
    "val_sampler = SequentialSampler(val_data)\n",
    "\n",
    "# dataLoader for validation set\n",
    "val_dataloader = DataLoader(val_data, sampler = val_sampler, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# freeze all the parameters\n",
    "for param in bert.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERT_Arch(nn.Module):\n",
    "\n",
    "    def __init__(self, bert):\n",
    "        \n",
    "      \n",
    "        super(BERT_Arch, self).__init__()\n",
    "\n",
    "        self.bert = bert \n",
    "      \n",
    "        # dropout layer\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "      \n",
    "        # relu activation function\n",
    "        self.relu =  nn.ReLU()\n",
    "\n",
    "        # dense layer 1\n",
    "        self.fc1 = nn.Linear(768,30)\n",
    "      \n",
    "        # dense layer 2 \n",
    "        self.fc2 = nn.Linear(30,8)\n",
    "        \n",
    "#         # dense layer 3\n",
    "#         self.fc3 = nn.Linear(256,64)\n",
    "      \n",
    "#         # dense layer 4 (Output layer)\n",
    "#         self.fc4 = nn.Linear(64,8)\n",
    "\n",
    "        #softmax activation function\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    #define the forward pass\n",
    "    def forward(self, sent_id, mask):\n",
    "\n",
    "        #pass the inputs to the model  \n",
    "        _, cls_hs = self.bert(sent_id, attention_mask=mask)\n",
    "      \n",
    "        x = self.fc1(cls_hs)\n",
    "        \n",
    "        x = self.relu(x)\n",
    "        \n",
    "        x = self.dropout(x)\n",
    "\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        x = self.relu(x)\n",
    "\n",
    "#         x = self.fc3(x)\n",
    "\n",
    "#         x = self.relu(x)\n",
    "\n",
    "#         # output layer\n",
    "#         x = self.fc4(x)\n",
    "      \n",
    "        # apply softmax activation\n",
    "        x = self.softmax(x)\n",
    "\n",
    "        return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pass the pre-trained BERT to our define architecture\n",
    "model = BERT_Arch(bert)\n",
    "\n",
    "# push the model to GPU\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer from hugging face transformers\n",
    "from transformers import AdamW\n",
    "\n",
    "# define the optimizer\n",
    "optimizer = AdamW(model.parameters(), lr = 1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.60714286 1.07142857 0.60810811 1.07142857 1.5        0.9375\n",
      " 0.9        0.97826087]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "f:\\ds git projects\\aml-1\\aml-1-venv\\lib\\site-packages\\sklearn\\utils\\validation.py:72: FutureWarning: Pass classes=[0 1 2 3 4 5 6 7], y=[4 7 2 4 0 1 4 5 2 1 2 4 4 3 3 1 7 2 3 6 2 7 1 6 3 2 2 4 3 3 5 5 6 5 2 2 3\n",
      " 1 2 6 1 6 3 2 1 5 4 6 2 6 4 1 6 7 2 5 2 7 5 5 3 2 2 2 5 2 5 7 7 6 5 2 2 3\n",
      " 7 6 2 0 4 7 4 3 6 0 7 3 5 3 7 3 1 2 1 7 6 7 5 6 6 2 6 7 1 7 2 0 2 5 0 3 3\n",
      " 5 1 5 4 7 6 0 3 0 2 7 2 5 3 0 6 0 3 5 1 2 5 6 2 6 7 1 5 6 2 2 7 0 1 1 4 6\n",
      " 0 7 4 2 2 3 0 6 6 4 3 2 6 2 1 7 7 1 2 0 7 1 1 4 5 5 1 0 6 5 2 5] as keyword args. From version 1.0 (renaming of 0.25) passing these as positional arguments will result in an error\n",
      "  \"will result in an error\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "#compute the class weights\n",
    "class_wts = compute_class_weight('balanced', np.unique(y_train), y_train)\n",
    "\n",
    "print(class_wts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# convert class weights to tensor\n",
    "weights= torch.tensor(class_wts,dtype=torch.float)\n",
    "weights = weights.to(device)\n",
    "\n",
    "# loss function\n",
    "cross_entropy  = nn.NLLLoss(weight=weights) \n",
    "# cross_entropy  = nn.NLLLoss() \n",
    "# number of training epochs\n",
    "epochs = 100\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to train the model\n",
    "def train():\n",
    "    model.train()\n",
    "    total_loss, total_accuracy = 0, 0\n",
    "  \n",
    "  # empty list to save model predictions\n",
    "    total_preds=[]\n",
    "  \n",
    "  # iterate over batches\n",
    "    for step,batch in enumerate(train_dataloader):\n",
    "        # progress update after every 50 batches.\n",
    "        if step % 50 == 0 and not step == 0:\n",
    "            print('  Batch {:>5,}  of  {:>5,}.'.format(step, len(train_dataloader)))\n",
    "\n",
    "        # push the batch to gpu\n",
    "        batch = [r.to(device) for r in batch]\n",
    "\n",
    "        sent_id, mask, labels = batch\n",
    "\n",
    "        # clear previously calculated gradients \n",
    "        model.zero_grad()        \n",
    "\n",
    "        # get model predictions for the current batch\n",
    "        preds = model(sent_id, mask)\n",
    "\n",
    "        # compute the loss between actual and predicted values\n",
    "        loss = cross_entropy(preds, labels.to(torch.long))\n",
    "\n",
    "        # add on to the total loss\n",
    "        total_loss = total_loss + loss.item()\n",
    "\n",
    "        # backward pass to calculate the gradients\n",
    "        loss.backward()\n",
    "\n",
    "        # clip the the gradients to 1.0. It helps in preventing the exploding gradient problem\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "        # update parameters\n",
    "        optimizer.step()\n",
    "\n",
    "        # model predictions are stored on GPU. So, push it to CPU\n",
    "        preds=preds.detach().cpu().numpy()\n",
    "\n",
    "        # append the model predictions\n",
    "        total_preds.append(preds)\n",
    "\n",
    "    # compute the training loss of the epoch\n",
    "    avg_loss = total_loss / len(train_dataloader)\n",
    "  \n",
    "    # predictions are in the form of (no. of batches, size of batch, no. of classes).\n",
    "    # reshape the predictions in form of (number of samples, no. of classes)\n",
    "    total_preds  = np.concatenate(total_preds, axis=0)\n",
    "\n",
    "    #returns the loss and predictions\n",
    "    return avg_loss, total_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for evaluating the model\n",
    "def evaluate():\n",
    "    print(\"\\nEvaluating...\")\n",
    "  \n",
    "  # deactivate dropout layers\n",
    "    model.eval()\n",
    "    \n",
    "    total_loss, total_accuracy = 0, 0\n",
    "  \n",
    "  # empty list to save the model predictions\n",
    "    total_preds = []\n",
    "\n",
    "  # iterate over batches\n",
    "    for step,batch in enumerate(val_dataloader):\n",
    "    \n",
    "        # Progress update every 50 batches.\n",
    "        if step % 50 == 0 and not step == 0:\n",
    "      \n",
    "            # Calculate elapsed time in minutes.\n",
    "            elapsed = format_time(time.time() - t0)\n",
    "            \n",
    "            # Report progress.\n",
    "            print('  Batch {:>5,}  of  {:>5,}.'.format(step, len(val_dataloader)))\n",
    "\n",
    "    # push the batch to gpu\n",
    "    batch = [t.to(device) for t in batch]\n",
    "\n",
    "    sent_id, mask, labels = batch\n",
    "\n",
    "    # deactivate autograd\n",
    "    with torch.no_grad():\n",
    "        # model predictions\n",
    "        preds = model(sent_id, mask)\n",
    "        # compute the validation loss between actual and predicted values\n",
    "        loss = cross_entropy(preds,labels)\n",
    "        \n",
    "        total_loss = total_loss + loss.item()\n",
    "\n",
    "        preds = preds.detach().cpu().numpy()\n",
    "\n",
    "        total_preds.append(preds)\n",
    "\n",
    "    # compute the validation loss of the epoch\n",
    "    avg_loss = total_loss / len(val_dataloader) \n",
    "\n",
    "    # reshape the predictions in form of (number of samples, no. of classes)\n",
    "    total_preds  = np.concatenate(total_preds, axis=0)\n",
    "\n",
    "    return avg_loss, total_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Epoch 1 / 100\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 1.913\n",
      "Validation Loss: 0.945\n",
      "\n",
      " Epoch 2 / 100\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 1.864\n",
      "Validation Loss: 0.959\n",
      "\n",
      " Epoch 3 / 100\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 1.882\n",
      "Validation Loss: 0.996\n",
      "\n",
      " Epoch 4 / 100\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 1.838\n",
      "Validation Loss: 0.991\n",
      "\n",
      " Epoch 5 / 100\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 1.781\n",
      "Validation Loss: 0.946\n",
      "\n",
      " Epoch 6 / 100\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 1.803\n",
      "Validation Loss: 0.963\n",
      "\n",
      " Epoch 7 / 100\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 1.800\n",
      "Validation Loss: 0.965\n",
      "\n",
      " Epoch 8 / 100\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 1.856\n",
      "Validation Loss: 0.963\n",
      "\n",
      " Epoch 9 / 100\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 1.835\n",
      "Validation Loss: 0.953\n",
      "\n",
      " Epoch 10 / 100\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 1.813\n",
      "Validation Loss: 0.967\n",
      "\n",
      " Epoch 11 / 100\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 1.819\n",
      "Validation Loss: 0.974\n",
      "\n",
      " Epoch 12 / 100\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 1.812\n",
      "Validation Loss: 0.945\n",
      "\n",
      " Epoch 13 / 100\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 1.787\n",
      "Validation Loss: 0.959\n",
      "\n",
      " Epoch 14 / 100\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 1.785\n",
      "Validation Loss: 0.987\n",
      "\n",
      " Epoch 15 / 100\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 1.786\n",
      "Validation Loss: 0.940\n",
      "\n",
      " Epoch 16 / 100\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 1.815\n",
      "Validation Loss: 0.923\n",
      "\n",
      " Epoch 17 / 100\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 1.803\n",
      "Validation Loss: 0.977\n",
      "\n",
      " Epoch 18 / 100\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 1.755\n",
      "Validation Loss: 0.956\n",
      "\n",
      " Epoch 19 / 100\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 1.726\n",
      "Validation Loss: 0.950\n",
      "\n",
      " Epoch 20 / 100\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 1.740\n",
      "Validation Loss: 0.964\n",
      "\n",
      " Epoch 21 / 100\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 1.797\n",
      "Validation Loss: 0.962\n",
      "\n",
      " Epoch 22 / 100\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 1.786\n",
      "Validation Loss: 0.930\n",
      "\n",
      " Epoch 23 / 100\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 1.733\n",
      "Validation Loss: 0.941\n",
      "\n",
      " Epoch 24 / 100\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 1.782\n",
      "Validation Loss: 0.961\n",
      "\n",
      " Epoch 25 / 100\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 1.701\n",
      "Validation Loss: 0.958\n",
      "\n",
      " Epoch 26 / 100\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 1.770\n",
      "Validation Loss: 0.950\n",
      "\n",
      " Epoch 27 / 100\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 1.684\n",
      "Validation Loss: 0.964\n",
      "\n",
      " Epoch 28 / 100\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 1.788\n",
      "Validation Loss: 0.932\n",
      "\n",
      " Epoch 29 / 100\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 1.746\n",
      "Validation Loss: 0.947\n",
      "\n",
      " Epoch 30 / 100\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 1.743\n",
      "Validation Loss: 0.975\n",
      "\n",
      " Epoch 31 / 100\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 1.777\n",
      "Validation Loss: 0.948\n",
      "\n",
      " Epoch 32 / 100\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 1.698\n",
      "Validation Loss: 0.939\n",
      "\n",
      " Epoch 33 / 100\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 1.671\n",
      "Validation Loss: 0.942\n",
      "\n",
      " Epoch 34 / 100\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 1.786\n",
      "Validation Loss: 0.938\n",
      "\n",
      " Epoch 35 / 100\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 1.718\n",
      "Validation Loss: 0.927\n",
      "\n",
      " Epoch 36 / 100\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 1.669\n",
      "Validation Loss: 0.918\n",
      "\n",
      " Epoch 37 / 100\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 1.711\n",
      "Validation Loss: 0.952\n",
      "\n",
      " Epoch 38 / 100\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 1.734\n",
      "Validation Loss: 0.908\n",
      "\n",
      " Epoch 39 / 100\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 1.662\n",
      "Validation Loss: 0.921\n",
      "\n",
      " Epoch 40 / 100\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 1.667\n",
      "Validation Loss: 0.935\n",
      "\n",
      " Epoch 41 / 100\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 1.723\n",
      "Validation Loss: 0.918\n",
      "\n",
      " Epoch 42 / 100\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 1.651\n",
      "Validation Loss: 0.927\n",
      "\n",
      " Epoch 43 / 100\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 1.643\n",
      "Validation Loss: 0.929\n",
      "\n",
      " Epoch 44 / 100\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 1.626\n",
      "Validation Loss: 0.938\n",
      "\n",
      " Epoch 45 / 100\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 1.671\n",
      "Validation Loss: 0.918\n",
      "\n",
      " Epoch 46 / 100\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 1.633\n",
      "Validation Loss: 0.922\n",
      "\n",
      " Epoch 47 / 100\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 1.657\n",
      "Validation Loss: 0.920\n",
      "\n",
      " Epoch 48 / 100\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 1.611\n",
      "Validation Loss: 0.935\n",
      "\n",
      " Epoch 49 / 100\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 1.641\n",
      "Validation Loss: 0.905\n",
      "\n",
      " Epoch 50 / 100\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 1.666\n",
      "Validation Loss: 0.908\n",
      "\n",
      " Epoch 51 / 100\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 1.548\n",
      "Validation Loss: 0.923\n",
      "\n",
      " Epoch 52 / 100\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 1.645\n",
      "Validation Loss: 0.928\n",
      "\n",
      " Epoch 53 / 100\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 1.637\n",
      "Validation Loss: 0.915\n",
      "\n",
      " Epoch 54 / 100\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 1.632\n",
      "Validation Loss: 0.933\n",
      "\n",
      " Epoch 55 / 100\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 1.620\n",
      "Validation Loss: 0.910\n",
      "\n",
      " Epoch 56 / 100\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 1.653\n",
      "Validation Loss: 0.902\n",
      "\n",
      " Epoch 57 / 100\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 1.623\n",
      "Validation Loss: 0.926\n",
      "\n",
      " Epoch 58 / 100\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 1.583\n",
      "Validation Loss: 0.915\n",
      "\n",
      " Epoch 59 / 100\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 1.622\n",
      "Validation Loss: 0.903\n",
      "\n",
      " Epoch 60 / 100\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 1.611\n",
      "Validation Loss: 0.918\n",
      "\n",
      " Epoch 61 / 100\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 1.559\n",
      "Validation Loss: 0.907\n",
      "\n",
      " Epoch 62 / 100\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 1.553\n",
      "Validation Loss: 0.890\n",
      "\n",
      " Epoch 63 / 100\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 1.514\n",
      "Validation Loss: 0.911\n",
      "\n",
      " Epoch 64 / 100\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 1.551\n",
      "Validation Loss: 0.916\n",
      "\n",
      " Epoch 65 / 100\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 1.555\n",
      "Validation Loss: 0.894\n",
      "\n",
      " Epoch 66 / 100\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 1.563\n",
      "Validation Loss: 0.926\n",
      "\n",
      " Epoch 67 / 100\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 1.619\n",
      "Validation Loss: 0.897\n",
      "\n",
      " Epoch 68 / 100\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 1.536\n",
      "Validation Loss: 0.907\n",
      "\n",
      " Epoch 69 / 100\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 1.568\n",
      "Validation Loss: 0.906\n",
      "\n",
      " Epoch 70 / 100\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 1.549\n",
      "Validation Loss: 0.902\n",
      "\n",
      " Epoch 71 / 100\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 1.538\n",
      "Validation Loss: 0.893\n",
      "\n",
      " Epoch 72 / 100\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 1.525\n",
      "Validation Loss: 0.898\n",
      "\n",
      " Epoch 73 / 100\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 1.517\n",
      "Validation Loss: 0.887\n",
      "\n",
      " Epoch 74 / 100\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 1.483\n",
      "Validation Loss: 0.882\n",
      "\n",
      " Epoch 75 / 100\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 1.499\n",
      "Validation Loss: 0.910\n",
      "\n",
      " Epoch 76 / 100\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 1.524\n",
      "Validation Loss: 0.896\n",
      "\n",
      " Epoch 77 / 100\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 1.530\n",
      "Validation Loss: 0.894\n",
      "\n",
      " Epoch 78 / 100\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 1.475\n",
      "Validation Loss: 0.912\n",
      "\n",
      " Epoch 79 / 100\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 1.440\n",
      "Validation Loss: 0.902\n",
      "\n",
      " Epoch 80 / 100\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 1.486\n",
      "Validation Loss: 0.884\n",
      "\n",
      " Epoch 81 / 100\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 1.514\n",
      "Validation Loss: 0.899\n",
      "\n",
      " Epoch 82 / 100\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 1.491\n",
      "Validation Loss: 0.891\n",
      "\n",
      " Epoch 83 / 100\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 1.507\n",
      "Validation Loss: 0.902\n",
      "\n",
      " Epoch 84 / 100\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 1.442\n",
      "Validation Loss: 0.901\n",
      "\n",
      " Epoch 85 / 100\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 1.501\n",
      "Validation Loss: 0.885\n",
      "\n",
      " Epoch 86 / 100\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 1.463\n",
      "Validation Loss: 0.881\n",
      "\n",
      " Epoch 87 / 100\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 1.400\n",
      "Validation Loss: 0.909\n",
      "\n",
      " Epoch 88 / 100\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 1.463\n",
      "Validation Loss: 0.893\n",
      "\n",
      " Epoch 89 / 100\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 1.424\n",
      "Validation Loss: 0.873\n",
      "\n",
      " Epoch 90 / 100\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 1.483\n",
      "Validation Loss: 0.901\n",
      "\n",
      " Epoch 91 / 100\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 1.449\n",
      "Validation Loss: 0.886\n",
      "\n",
      " Epoch 92 / 100\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 1.464\n",
      "Validation Loss: 0.868\n",
      "\n",
      " Epoch 93 / 100\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 1.431\n",
      "Validation Loss: 0.896\n",
      "\n",
      " Epoch 94 / 100\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 1.470\n",
      "Validation Loss: 0.905\n",
      "\n",
      " Epoch 95 / 100\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 1.462\n",
      "Validation Loss: 0.878\n",
      "\n",
      " Epoch 96 / 100\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 1.431\n",
      "Validation Loss: 0.897\n",
      "\n",
      " Epoch 97 / 100\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 1.426\n",
      "Validation Loss: 0.889\n",
      "\n",
      " Epoch 98 / 100\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 1.414\n",
      "Validation Loss: 0.858\n",
      "\n",
      " Epoch 99 / 100\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 1.459\n",
      "Validation Loss: 0.902\n",
      "\n",
      " Epoch 100 / 100\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 1.406\n",
      "Validation Loss: 0.901\n"
     ]
    }
   ],
   "source": [
    "# set initial loss to infinite\n",
    "best_test_loss = float('inf')\n",
    "\n",
    "# empty lists to store training and validation loss of each epoch\n",
    "train_losses=[]\n",
    "val_losses=[]\n",
    "\n",
    "#for each epoch\n",
    "for epoch in range(epochs):\n",
    "     \n",
    "    print('\\n Epoch {:} / {:}'.format(epoch + 1, epochs))\n",
    "    \n",
    "    #train model\n",
    "    train_loss, _ = train()\n",
    "    \n",
    "    #evaluate model\n",
    "    val_loss, _ = evaluate()\n",
    "    \n",
    "    #save the best model\n",
    "    if val_loss < best_test_loss:\n",
    "        best_test_loss = val_loss\n",
    "        torch.save(model.state_dict(), 'saved_weights.pt')\n",
    "    \n",
    "    # append training and validation loss\n",
    "    train_losses.append(train_loss)\n",
    "    val_losses.append(val_loss)\n",
    "    \n",
    "    print(f'\\nTraining Loss: {train_loss:.3f}')\n",
    "    print(f'Validation Loss: {val_loss:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#load weights of best model\n",
    "path = 'saved_weights.pt'\n",
    "model.load_state_dict(torch.load(path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get predictions for test data\n",
    "with torch.no_grad():\n",
    "    preds = model(test_seq.to(device), test_mask.to(device))\n",
    "    preds = preds.detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.33      0.75      0.46         4\n",
      "           1       0.40      0.86      0.55         7\n",
      "           2       0.70      0.58      0.64        12\n",
      "           3       0.00      0.00      0.00         7\n",
      "           4       0.33      0.80      0.47         5\n",
      "           5       0.14      0.12      0.13         8\n",
      "           6       0.00      0.00      0.00         9\n",
      "           7       0.86      0.75      0.80         8\n",
      "\n",
      "    accuracy                           0.45        60\n",
      "   macro avg       0.35      0.48      0.38        60\n",
      "weighted avg       0.37      0.45      0.39        60\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "f:\\ds git projects\\aml-1\\aml-1-venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "f:\\ds git projects\\aml-1\\aml-1-venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "f:\\ds git projects\\aml-1\\aml-1-venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "# model's performance\n",
    "preds = np.argmax(preds, axis = 1)\n",
    "print(classification_report(test_y, preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4, 4, 4, 0, 4, 4, 1, 7, 4, 5, 7, 1, 2, 0, 5, 1, 2, 7, 0, 5, 1, 0,\n",
       "       0, 1, 5, 5, 1, 2, 1, 1, 4, 7, 4, 4, 5, 1, 4, 2, 7, 5, 0, 0, 1, 2,\n",
       "       4, 2, 4, 1, 2, 1, 7, 0, 0, 2, 2, 1, 1, 1, 7, 2], dtype=int64)"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>col_0</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>7</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>row_0</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "col_0  0  1  2  4  5  7\n",
       "row_0                  \n",
       "0      3  0  0  0  1  0\n",
       "1      0  6  0  1  0  0\n",
       "2      0  0  7  3  2  0\n",
       "3      0  5  1  0  1  0\n",
       "4      1  0  0  4  0  0\n",
       "5      2  0  1  3  1  1\n",
       "6      2  3  1  1  2  0\n",
       "7      1  1  0  0  0  6"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# confusion matrix\n",
    "pd.crosstab(test_y, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3, 0, 0, 0, 0, 1, 0, 0],\n",
       "       [0, 6, 0, 0, 1, 0, 0, 0],\n",
       "       [0, 0, 7, 0, 3, 2, 0, 0],\n",
       "       [0, 5, 1, 0, 0, 1, 0, 0],\n",
       "       [1, 0, 0, 0, 4, 0, 0, 0],\n",
       "       [2, 0, 1, 0, 3, 1, 0, 1],\n",
       "       [2, 3, 1, 0, 1, 2, 0, 0],\n",
       "       [1, 1, 0, 0, 0, 0, 0, 6]], dtype=int64)"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y_test, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aml-1-venv",
   "language": "python",
   "name": "aml-1-venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
